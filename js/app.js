const TL=['Plug & Play','Guided Setup','Expert Required','Enterprise Only'],TK=['plug-and-play','guided-setup','expert-required','enterprise-only'];
function assess(r){
  let s=0,d=0,g=0,p=0;const cat=r.category||'',risk=(r.riskRaw||'').toLowerCase(),tags=(r.tags||[]).map(t=>t.toLowerCase()),desc=(r.desc||'').toLowerCase(),aud=(r.audience||'').toLowerCase();
  if(cat==='AI Red Teaming')s=Math.max(s,2);
  if(risk.includes('caution')||risk.includes('offensive'))s=3;
  if(tags.some(t=>['exploit','pentesting','injection','phishing','cracking'].includes(t)))s=Math.max(s,2);
  if(tags.some(t=>['cli','dev','devsecops','sdk'].includes(t)))s=Math.max(s,1);
  if(tags.some(t=>['framework','standard','protocol'].includes(t)))s=Math.max(s,1);
  if(tags.some(t=>['game','training'].includes(t)))s=0;
  if(aud==='builder')s=Math.max(s,1);
  if(tags.some(t=>['saas','platform'].includes(t))||desc.includes('platform'))d=Math.max(d,1);
  if(tags.includes('open source')&&(desc.includes('self-host')||desc.includes('deploy')))d=Math.max(d,2);
  if(tags.includes('enterprise'))d=Math.max(d,2);
  if(desc.includes('docker')||desc.includes('kubernetes'))d=Math.max(d,2);
  if(cat==='AI Governance & Standards'&&!tags.some(t=>['game','training','article','influencer'].includes(t)))g=Math.max(g,1);
  if(risk.includes('caution')||risk.includes('offensive'))g=Math.max(g,2);
  if(tags.some(t=>['enterprise','procurement'].includes(t)))g=Math.max(g,3);
  if(desc.includes('governance'))g=Math.max(g,1);
  if(risk.includes('critical')||risk.includes('red flag'))p=3;
  if(risk.includes('high')||risk.includes('privacy'))p=Math.max(p,2);
  if(risk.includes('medium'))p=Math.max(p,1);
  if(tags.some(t=>['shadow ai','privacy risk','dlp'].includes(t)))p=Math.max(p,2);
  if(tags.includes('china'))p=3;
  if(desc.includes('proprietary code')||desc.includes('data privacy'))p=Math.max(p,1);
  const total=s+d+g+p;let ti;if(total<=3)ti=0;else if(total<=6)ti=1;else if(total<=9)ti=2;else ti=3;
  return{tier:TL[ti],tierKey:TK[ti],scores:{skill:s,deployment:d,governance:g,privacy:p},total};
}
function parseTier(v){if(!v)return null;const l=v.trim().toLowerCase();for(let i=0;i<TL.length;i++){if(l===TL[i].toLowerCase()||l===TK[i])return i}return null}
const R=[
// === AI Red Teaming (19) ===
{name:"Gandalf (Lakera)",url:"https://gandalf.lakera.ai/agent-breaker",category:"AI Red Teaming",desc:"The world's most popular AI red teaming game. Learn prompt injection by doing it.",riskRaw:"Safe",audience:"Red Team",tags:["Training","Game","Injection"],llm:[],stages:["test"],agentic:false,backWhat:"An interactive browser game created by Lakera that teaches prompt injection through hands-on challenges. Players attempt to trick an AI guardian into revealing a secret password across increasingly difficult levels, each adding new defensive layers.",backSecurity:"The best way to understand prompt injection is to practice it. Gandalf builds intuition for how LLMs can be manipulated — knowledge that directly translates to writing better system prompts and understanding guardrail limitations. Recently expanded to include an Agent Breaker mode for testing agentic AI vulnerabilities.",backWhen:"Use as a training exercise for security teams new to AI threats. Excellent onboarding tool for red teamers transitioning from traditional AppSec to AI security. Zero setup, zero risk — just open the browser."},
{name:"HexStrike",url:"https://www.hexstrike.com/",category:"AI Red Teaming",desc:"Advanced AI red teaming platform for LLM vulnerabilities.",riskRaw:"Medium",audience:"Red Team",tags:["Pentesting","SaaS","Platform"],llm:["LLM01","LLM02","LLM06"],stages:["test"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A SaaS red teaming platform purpose-built for testing LLM vulnerabilities. Provides structured adversarial testing workflows with automated attack generation and results tracking.",backSecurity:"Covers prompt injection, data leakage, and output manipulation testing in a managed environment. The platform approach means you get consistent, repeatable tests rather than ad-hoc manual probing — critical for demonstrating coverage to auditors.",backWhen:"Use when you need structured, repeatable red team engagements against LLM-powered applications. Best suited for teams that want a managed platform rather than building their own testing harness from open-source tools."},
{name:"Medusa",url:"https://github.com/Pantheon-Security/medusa",category:"AI Red Teaming",desc:"Open-source framework for offensive AI testing and jailbreaking.",riskRaw:"Medium",audience:"Red Team",tags:["Jailbreak","Open Source","GitHub"],llm:["LLM01","LLM02"],stages:["test"],agentic:false,complexityOverride:"Expert Required",backWhat:"An open-source offensive AI testing framework from Pantheon Security. Provides a library of jailbreaking techniques, prompt injection payloads, and adversarial attack patterns against LLMs.",backSecurity:"Medusa aggregates known attack techniques into a reusable framework, making it easier to systematically test LLM defences. Covers jailbreak methods, role-playing attacks, and encoding-based bypasses that map directly to LLM01 and LLM02.",backWhen:"Use when you need granular control over offensive testing and want to extend or customise attack patterns. Requires Python expertise and familiarity with adversarial ML concepts. Not a point-and-click tool — this is for hands-on red teamers."},
{name:"BlackIce (Databricks)",url:"https://www.databricks.com/blog/announcing-blackice-containerized-red-teaming-toolkit-ai-security-testing",category:"AI Red Teaming",desc:"Containerized red teaming toolkit for AI security testing.",riskRaw:"Medium",audience:"Red Team",tags:["Container","Pentesting","Tool"],llm:["LLM01","LLM02","LLM06"],stages:["test"],agentic:false,complexityOverride:"Expert Required",backWhat:"A containerised red teaming toolkit from Databricks designed for AI security testing. Ships as Docker containers with pre-configured testing environments and attack tooling.",backSecurity:"The containerised approach ensures reproducible test environments — critical when you need to demonstrate consistent findings across engagements. Integrates with Databricks\' ML ecosystem but can be used standalone against any LLM endpoint.",backWhen:"Use when you need isolated, reproducible red team environments, particularly in regulated industries where test environment consistency matters. Requires Docker knowledge and red team experience."},
{name:"Purple Llama (Meta)",url:"https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/",category:"AI Red Teaming",desc:"Open trust and safety tools for evaluating generative AI. Includes CyberSecEval benchmarks.",riskRaw:"Safe",audience:"Red Team",tags:["Safety","Eval","Meta","Open Source"],llm:["LLM01","LLM02","LLM07","LLM09"],stages:["test","develop"],agentic:false,complexityOverride:"Guided Setup",backWhat:"Meta\'s open-source trust and safety toolkit for evaluating generative AI systems. Includes CyberSecEval benchmarks for measuring LLM security, Llama Guard for content classification, and Code Shield for detecting insecure code generation.",backSecurity:"CyberSecEval is one of the few standardised benchmarks for measuring LLM security posture. It tests for prompt injection susceptibility, insecure code generation, and cybersecurity knowledge. Llama Guard provides a practical content safety classifier that can be deployed as a guardrail layer.",backWhen:"Use during model evaluation to benchmark security properties before deployment. CyberSecEval gives you comparable metrics across different models. Llama Guard is useful as a building block for content safety pipelines."},
{name:"Promptfoo",url:"https://www.promptfoo.dev/",category:"AI Red Teaming",desc:"CLI tool for testing, red teaming, and evaluating LLM prompts. Extensible with custom plugins.",riskRaw:"Safe",audience:"Builder",tags:["Testing","CLI","Dev","Open Source"],llm:["LLM01","LLM02","LLM06"],stages:["test","develop"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A CLI tool for testing, evaluating, and red teaming LLM prompts. Supports custom test suites in YAML, automated red teaming with plugin-based attack generation, and side-by-side model comparison. Extensible with custom plugins and assertions.",backSecurity:"Promptfoo bridges the gap between development-time testing and security evaluation. Its red teaming mode generates adversarial inputs automatically, while its evaluation framework lets you define security-specific assertions (no PII leakage, no jailbreak success, output format compliance).",backWhen:"Use as part of your CI/CD pipeline to catch prompt injection vulnerabilities and output safety issues before deployment. Excellent for teams that want to shift security testing left without building custom tooling. The YAML-based config makes it accessible to security engineers who aren\'t ML specialists."},
{name:"AIVSS",url:"https://aivss.parthsohaney.online/",category:"AI Red Teaming",desc:"AI Vulnerability Scoring System for standardised AI risk rating.",riskRaw:"Safe",audience:"Red Team",tags:["Framework","Scoring","Risk"],llm:[],stages:["scope"],agentic:false,backWhat:"The AI Vulnerability Scoring System — a web-based calculator that provides standardised risk ratings for AI-specific vulnerabilities, similar to how CVSS scores traditional software vulnerabilities.",backSecurity:"Brings much-needed standardisation to AI vulnerability assessment. Without a common scoring framework, teams struggle to prioritise AI security findings consistently. AIVSS provides a structured approach to rating severity across dimensions specific to AI systems.",backWhen:"Use when you need to communicate AI vulnerability severity to stakeholders in a standardised way. Particularly useful for GRC teams building AI risk registers who need consistent scoring across different types of AI findings."},
{name:"Arcanum Security Context",url:"https://arcanum-sec.github.io/sec-context/",category:"AI Red Teaming",desc:"Security context and research repository for AI vulnerabilities.",riskRaw:"Safe",audience:"Red Team",tags:["Research","Docs","Reference"],llm:[],stages:["scope"],agentic:false,backWhat:"A security context and research repository maintained on GitHub that catalogues AI vulnerabilities, attack patterns, and defensive techniques. Serves as a reference library for AI security practitioners.",backSecurity:"Functions as a curated knowledge base that maps real-world AI vulnerabilities to defensive strategies. Useful for understanding the current threat landscape and finding relevant research when investigating specific attack vectors.",backWhen:"Use as a reference resource when researching specific AI vulnerabilities or building threat models. Good starting point for teams new to AI security who need to understand the landscape of known attack patterns."},
{name:"XM Cyber",url:"https://www.xmcyber.com/",category:"AI Red Teaming",desc:"Breach and Attack Simulation — maps attack paths across hybrid infrastructure including AI workloads.",riskRaw:"Safe",audience:"Red Team",tags:["BAS","Simulation","Cloud"],llm:[],stages:["test","monitor"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"An enterprise Breach and Attack Simulation (BAS) platform that continuously maps attack paths across hybrid infrastructure. Models how an attacker could move laterally through networks, including paths that traverse AI workloads and ML infrastructure.",backSecurity:"XM Cyber\'s value for AI security is in understanding how AI infrastructure fits into the broader attack surface. ML training pipelines, model serving endpoints, and data stores create new lateral movement paths that traditional vulnerability scanning misses.",backWhen:"Use when you need to understand how AI infrastructure affects your overall attack surface. Enterprise deployment requiring procurement, network access, and agent installation across infrastructure. Not an AI-specific tool, but increasingly relevant as AI workloads expand."},
{name:"GoPhish",url:"https://getgophish.com/",category:"AI Red Teaming",desc:"Open-source phishing toolkit. Relevant for testing AI-generated phishing campaigns.",riskRaw:"Caution (Offensive)",audience:"Red Team",tags:["Phishing","Social Engineering","Testing"],llm:[],stages:["test"],agentic:false,complexityOverride:"Expert Required",backWhat:"An open-source phishing simulation toolkit for running controlled social engineering campaigns. Provides campaign management, email template creation, landing page cloning, and results tracking.",backSecurity:"Relevant to AI security because LLMs are transforming phishing attacks — generating more convincing, personalised, and scalable social engineering content. GoPhish lets you test whether your organisation can detect AI-generated phishing, and whether your AI-powered email security actually catches sophisticated campaigns.",backWhen:"Use when testing your organisation\'s resilience against AI-enhanced phishing attacks. Requires a mail server, careful scoping, and offensive security expertise. Always get written authorisation before running campaigns."},
{name:"BurpSuite",url:"https://portswigger.net/burp",category:"AI Red Teaming",desc:"Web vulnerability scanner — the standard for testing the web layer of AI applications.",riskRaw:"Safe",audience:"Red Team",tags:["Web Sec","Scanner","AppSec"],llm:["LLM02"],stages:["test","develop"],agentic:false,complexityOverride:"Guided Setup",backWhat:"PortSwigger\'s industry-standard web application security testing platform. Provides intercepting proxy, vulnerability scanner, and extensible testing framework for web applications.",backSecurity:"AI applications are web applications. Every LLM-powered product has an HTTP layer that can be tested with traditional AppSec tools. BurpSuite is essential for testing the web surface of AI applications — API endpoints, authentication, session management, and the interaction layer between users and LLM backends.",backWhen:"Use alongside AI-specific tools to test the full attack surface of LLM-powered applications. The AI-specific red teaming tools test the model layer; BurpSuite tests everything around it. Most AI security assessments should include both."},
{name:"Husn Canary",url:"https://www.husncanary.com",category:"AI Red Teaming",desc:"Canary tokens designed specifically for AI model data leakage detection.",riskRaw:"Safe",audience:"Blue Team",tags:["Canary","DLP","Detection"],llm:["LLM06"],stages:["monitor","operate"],agentic:false,complexityOverride:"Guided Setup",backWhat:"Canary tokens designed specifically for detecting AI model data leakage. Creates unique, trackable tokens that can be embedded in training data, documents, or knowledge bases to detect when an AI system exposes them.",backSecurity:"Addresses LLM06 (Sensitive Information Disclosure) through detection rather than prevention. If your canary token appears in an LLM\'s output, you have concrete evidence that the model has been trained on or has access to your data. This is particularly valuable for detecting unauthorised training data usage.",backWhen:"Use as a detection layer alongside guardrails. Embed canary tokens in sensitive documents before they enter RAG pipelines or training datasets. Monitor for token exposure in LLM outputs. Lightweight to deploy but requires a monitoring strategy."},
{name:"Garak",url:"https://garak.ai/",category:"AI Red Teaming",desc:"Leading open-source LLM vulnerability scanner. Probes for prompt injection, data leakage, hallucination. 1.2k GitHub stars.",riskRaw:"Safe",audience:"Red Team",tags:["Vuln Scanner","Open Source","CLI"],llm:["LLM01","LLM02","LLM06","LLM09"],stages:["test"],agentic:false,complexityOverride:"Guided Setup",backWhat:"The leading open-source LLM vulnerability scanner. Systematically probes language models for prompt injection, data leakage, hallucination, and other vulnerabilities using a library of configurable attack probes and detectors.",backSecurity:"Garak is the closest thing to an automated vulnerability scanner for LLMs. It maps directly to the OWASP LLM Top 10, testing for prompt injection (LLM01), insecure output handling (LLM02), information disclosure (LLM06), and overreliance (LLM09). Results are structured and reportable.",backWhen:"Use as a baseline security scan for any LLM deployment. Run it during development to catch obvious vulnerabilities, and periodically in production as models are updated. Requires Python and API access to target models, but the scan configuration is straightforward."},
{name:"HarmBench",url:"https://www.harmbench.org/",category:"AI Red Teaming",desc:"Automated red teaming and robust refusal evaluation framework. Academic benchmark for adversarial robustness.",riskRaw:"Safe",audience:"Red Team",tags:["Benchmark","Open Source","Eval"],llm:["LLM01","LLM02","LLM03","LLM06","LLM09"],stages:["test"],agentic:false,complexityOverride:"Expert Required",backWhat:"An academic benchmark framework for evaluating adversarial robustness of language models. Provides standardised evaluation of both attack methods and defence mechanisms, with automated red teaming capabilities across multiple attack vectors.",backSecurity:"HarmBench offers the most rigorous academic evaluation of LLM safety. It tests models against a curated set of harmful behaviours and measures both the success rate of attacks and the robustness of refusals. Useful for comparing model safety properties before procurement decisions.",backWhen:"Use when you need academic-grade evaluation of model robustness, particularly for model selection decisions. Requires GPU infrastructure, model loading expertise, and familiarity with evaluation pipelines. Not a quick scan — this is deep evaluation work."},
{name:"Giskard",url:"https://www.giskard.ai/",category:"AI Red Teaming",desc:"Open-source LLM testing for vulnerabilities, bias, and hallucination. Growing alternative to commercial red teaming.",riskRaw:"Safe",audience:"Builder",tags:["Testing","Bias","Open Source"],llm:["LLM01","LLM02","LLM05"],stages:["test"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An open-source testing framework for LLM vulnerabilities, bias, and hallucination. Provides automated test generation, vulnerability scanning, and continuous testing integration with a focus on both security and fairness.",backSecurity:"Giskard uniquely combines security testing (prompt injection, data leakage) with fairness and bias evaluation. This dual focus is valuable because regulatory frameworks like the EU AI Act require both security and fairness assessments. One tool covering both reduces integration complexity.",backWhen:"Use when you need to evaluate both security vulnerabilities and bias/fairness issues, particularly for EU AI Act compliance. Good for teams that want a growing open-source alternative to commercial red teaming platforms."},
{name:"Mindgard",url:"https://mindgard.ai/",category:"AI Red Teaming",desc:"Continuous AI DAST — finds runtime-only AI vulnerabilities through dynamic application security testing.",riskRaw:"Safe",audience:"Red Team",tags:["DAST","Pentesting","Platform"],llm:["LLM01","LLM02","LLM04","LLM06","LLM09"],stages:["test"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A continuous AI Dynamic Application Security Testing (DAST) platform. Discovers runtime-only vulnerabilities in AI applications through dynamic testing against live endpoints, similar to how traditional DAST tools test web applications.",backSecurity:"Mindgard brings the DAST paradigm to AI security — testing running applications rather than static models. This catches vulnerabilities that only manifest at runtime: prompt injection through specific input combinations, data leakage under load, and denial-of-service through adversarial inputs.",backWhen:"Use for continuous security testing of deployed AI applications. The DAST approach complements static analysis tools like Garak. Best suited for teams with established DevSecOps practices who want to add AI testing to their existing pipeline."},
{name:"Adversa AI",url:"https://adversa.ai/",category:"AI Red Teaming",desc:"Red teaming platform for LLMs with automated benchmarking. Covers prompt injection and agentic threats.",riskRaw:"Safe",audience:"Red Team",tags:["Platform","Benchmark","SaaS"],llm:["LLM01","LLM02","LLM06"],stages:["test"],agentic:true,complexityOverride:"Guided Setup",backWhat:"A red teaming platform for LLMs with automated benchmarking and adversarial testing capabilities. Covers prompt injection, data extraction, jailbreaking, and increasingly agentic AI threats.",backSecurity:"Adversa AI provides structured red teaming with benchmarking — meaning you get not just vulnerability findings but comparative scores against baseline safety metrics. Their coverage of agentic threats is notable as most tools haven\'t caught up to agent-specific attack vectors yet.",backWhen:"Use when you need both red teaming findings and benchmark scores for reporting to leadership. The agentic coverage makes it relevant for teams deploying AI agents. SaaS platform with guided setup — lower barrier than self-hosted alternatives."},
{name:"Agentic Radar",url:"https://github.com/splxai/agentic-radar",category:"AI Red Teaming",desc:"First open-source agentic security scanner. Agent scanning, red teaming, multi-agent simulation.",riskRaw:"Safe",audience:"Red Team",tags:["Agentic","Open Source","Scanner"],llm:["LLM01","LLM07"],stages:["test","develop"],agentic:true,complexityOverride:"Guided Setup",backWhat:"The first open-source security scanner purpose-built for agentic AI systems. Analyses agent configurations, scans for insecure tool bindings, and simulates multi-agent attack scenarios. From Splx AI.",backSecurity:"Addresses a critical gap — most AI security tools focus on single-model interactions, but agentic systems introduce new attack surfaces: insecure tool use (LLM07), excessive agency (LLM08), and inter-agent manipulation. Agentic Radar scans for these agent-specific vulnerabilities.",backWhen:"Use when deploying AI agents that use tools, access APIs, or interact with other agents. Essential for any MCP-based architecture or multi-agent system. Open-source and actively maintained, but requires Python setup and familiarity with agent architectures."},
{name:"Prompt Fuzzer",url:"https://github.com/prompt-security/ps-fuzz",category:"AI Red Teaming",desc:"Open-source interactive prompt resilience testing tool by Prompt Security.",riskRaw:"Safe",audience:"Red Team",tags:["Fuzzing","Open Source","CLI"],llm:["LLM01","LLM02","LLM06"],stages:["test"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An open-source interactive prompt resilience testing tool from Prompt Security. Generates and tests adversarial inputs against LLM system prompts to evaluate their robustness against injection and manipulation attacks.",backSecurity:"Focused specifically on system prompt resilience — testing whether an LLM\'s instructions can be overridden, extracted, or bypassed. This is the core of LLM01 (Prompt Injection) and one of the most common real-world attack vectors.",backWhen:"Use when hardening system prompts before deployment. Run against your production prompts to find injection vectors, then iterate on prompt design. Quick to set up via CLI, but requires understanding of prompt injection techniques to interpret results effectively."},
// === AI Governance & Standards (15) ===
{name:"OWASP AI Exchange",url:"https://owaspai.org/",category:"AI Governance & Standards",desc:"The comprehensive open-source guide to AI security — the definitive reference.",riskRaw:"Safe",audience:"All",tags:["Standard","OWASP","Reference"],llm:["LLM01","LLM02","LLM03","LLM04","LLM05","LLM06","LLM07","LLM08","LLM09","LLM10"],stages:["scope","govern"],agentic:false,backWhat:"The comprehensive open-source guide to AI security maintained by OWASP. Covers threats, controls, and governance across the entire AI lifecycle. The definitive reference for understanding the AI threat landscape.",backSecurity:"This is the single most important reference document in AI security. It maps threats across all 10 LLM risks, provides control recommendations, and connects to the broader OWASP ecosystem. If you read one resource, make it this one.",backWhen:"Use as your foundational reference when building AI security programmes, writing policies, or assessing risks. Consult it when mapping controls to specific threats. Updated regularly by the OWASP community."},
{name:"OWASP GenAI Solutions",url:"https://genai.owasp.org/ai-security-solutions-landscape/",category:"AI Governance & Standards",desc:"OWASP Solutions Landscape directory for AI security vendors.",riskRaw:"Safe",audience:"All",tags:["Directory","OWASP","Vendor"],llm:[],stages:["scope"],agentic:false,backWhat:"OWASP\'s curated Solutions Landscape directory mapping AI security vendors and tools to specific GenAI security use cases. Part of the broader OWASP GenAI Security Project.",backSecurity:"Provides an OWASP-endorsed view of the vendor landscape, helping teams identify solutions that map to specific OWASP risks. Useful as a cross-reference when evaluating tools — if a vendor appears in both Yuntona and the OWASP directory, that\'s a stronger signal.",backWhen:"Use when evaluating vendors to cross-reference against OWASP\'s own assessment. Helpful for procurement justification — being listed in an OWASP directory carries weight with auditors and compliance teams."},
{name:"ISO/IEC 42001",url:"https://www.iso.org/standard/56641.html",category:"AI Governance & Standards",desc:"The global standard for AI Management Systems.",riskRaw:"Safe",audience:"Blue Team",tags:["Compliance","Standard","ISO"],llm:[],stages:["govern"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"The global standard for AI Management Systems, published by ISO/IEC. Specifies requirements for establishing, implementing, maintaining, and continually improving an AI management system within organisations.",backSecurity:"ISO 42001 is becoming the gold standard for demonstrating AI governance maturity. It provides a structured framework for managing AI risks, similar to how ISO 27001 frames information security. Certification demonstrates to customers, regulators, and partners that your AI practices meet international standards.",backWhen:"Pursue when your organisation needs to demonstrate AI governance maturity to enterprise customers, regulators, or partners. This is an organisational commitment requiring cross-functional implementation, internal audit capability, and ongoing maintenance. Plan for 6-12 months minimum."},
{name:"ETSI AI Security Standard",url:"https://www.etsi.org/newsroom/press-releases/2627-etsi-releases-world-leading-standard-for-securing-ai",category:"AI Governance & Standards",desc:"World-leading standard (TS 104 223) for securing AI systems.",riskRaw:"Safe",audience:"Blue Team",tags:["Standard","ETSI","Compliance"],llm:[],stages:["govern"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"The European Telecommunications Standards Institute\'s standard (TS 104 223) for securing AI systems. One of the first formal technical standards specifically addressing AI security requirements.",backSecurity:"Provides prescriptive technical requirements for AI security rather than just principles. Covers secure development, deployment, and operation of AI systems with specific controls. Particularly relevant for organisations operating in European markets or regulated industries.",backWhen:"Adopt when building AI systems for European markets or regulated sectors. The standard complements ISO 42001 (management system) with technical security requirements. Enterprise-level commitment requiring process changes and potentially external assessment."},
{name:"EU AI Act Practical Guide",url:"https://www.cuatrecasas.com/en/global/intellectual-property/art/eu-ai-act-practical-guide",category:"AI Governance & Standards",desc:"Legal framework guide for navigating European AI regulations.",riskRaw:"Safe",audience:"All",tags:["Legal","EU","Regulation"],llm:[],stages:["govern"],agentic:false,backWhat:"A practical legal guide by Cuatrecasas for navigating the European AI regulation. Breaks down the EU AI Act\'s risk classification system, prohibited practices, transparency requirements, and compliance obligations into actionable guidance.",backSecurity:"The EU AI Act is the most comprehensive AI regulation globally and will affect any organisation deploying AI in European markets. Understanding the risk classification system (unacceptable, high, limited, minimal risk) is essential for determining what security and governance controls are legally required.",backWhen:"Read when your organisation deploys AI systems that affect EU citizens or operates in European markets. Essential background for GRC teams building AI compliance programmes. The guide makes the dense regulation accessible without requiring legal expertise."},
{name:"NIST AI RMF Maturity Model",url:"https://arxiv.org/abs/2401.15229",category:"AI Governance & Standards",desc:"Framework for assessing AI risk maturity based on NIST standards.",riskRaw:"Safe",audience:"Blue Team",tags:["Framework","NIST","Risk"],llm:[],stages:["scope","govern"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A framework for assessing organisational maturity in AI risk management, based on NIST\'s AI Risk Management Framework. Provides a structured approach to measuring and improving AI governance capabilities across multiple dimensions.",backSecurity:"Maps AI risk management capabilities to maturity levels, helping organisations understand where they stand and what to prioritise. Useful for building a roadmap from ad-hoc AI usage to structured AI governance, with clear milestones along the way.",backWhen:"Use when building an AI governance roadmap or assessing current maturity for leadership reporting. Requires mapping to your organisation\'s specific processes and capabilities — not just reading the framework but applying it to your context."},
{name:"Witness.ai",url:"https://witness.ai/",category:"AI Governance & Standards",desc:"Enterprise AI safety platform providing visibility and policy enforcement for shadow AI.",riskRaw:"Safe",audience:"Blue Team",tags:["Shadow AI","Policy","Enterprise"],llm:["LLM06"],stages:["govern","monitor"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"An enterprise AI safety platform that provides visibility and policy enforcement for shadow AI usage across the organisation. Monitors employee interactions with AI tools and enforces data protection policies in real-time.",backSecurity:"Shadow AI is one of the most immediate risks for enterprises — employees using ChatGPT, Claude, and other AI tools with sensitive data, bypassing security controls. Witness.ai provides visibility into this usage and enforcement of data protection policies without blocking productivity.",backWhen:"Deploy when shadow AI is an identified risk and you need both visibility and policy enforcement. Enterprise platform requiring procurement, org-wide deployment, and policy configuration. Most valuable in regulated industries with strict data handling requirements."},
{name:"Harmonic Security",url:"https://www.harmonic.security/",category:"AI Governance & Standards",desc:"Data protection platform that detects and governs Shadow AI usage across the enterprise.",riskRaw:"Safe",audience:"Blue Team",tags:["DLP","Shadow AI","Enterprise"],llm:["LLM06"],stages:["govern","monitor"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"A data protection platform specifically designed for governing Shadow AI usage in enterprises. Detects AI tool usage, classifies data being shared with AI services, and enforces data loss prevention policies across AI interactions.",backSecurity:"Addresses the DLP gap that traditional tools miss — data leaving through AI chat interfaces, API calls to LLM providers, and embedded AI features in productivity tools. Harmonic tracks what data goes where and enforces policies to prevent sensitive information from reaching external AI services.",backWhen:"Deploy when you need enterprise-wide DLP specifically for AI channels. Procurement, integration with existing DLP infrastructure, and organisation-wide rollout are required. Complements traditional DLP rather than replacing it."},
{name:"Zenity",url:"https://zenity.io/",category:"AI Governance & Standards",desc:"Governance and security for Low-Code/No-Code AI agents and copilots.",riskRaw:"Safe",audience:"Blue Team",tags:["Low-Code","Governance","Copilot"],llm:["LLM01","LLM07"],stages:["govern","operate"],agentic:true,complexityOverride:"Enterprise Only",backWhat:"A governance and security platform for Low-Code/No-Code AI agents and copilots. Provides visibility, risk assessment, and policy enforcement for AI-powered automations built by business users rather than developers.",backSecurity:"The Low-Code/No-Code AI explosion means business users are building AI agents without security review. Zenity provides governance for this shadow development — discovering AI automations, assessing their risk, and enforcing policies without blocking citizen developer productivity.",backWhen:"Deploy when business users are building AI-powered automations in platforms like Power Platform, Zapier, or custom copilots. Enterprise platform requiring procurement and integration with your Low-Code/No-Code ecosystem."},
{name:"Lethal Trifecta (Simon Willison)",url:"https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/",category:"AI Governance & Standards",desc:"Essential article on Prompt Injection + Tool Use + Permissions — the core AI threat model.",riskRaw:"Safe",audience:"All",tags:["Education","Article","Risk"],llm:["LLM01","LLM07","LLM08"],stages:["scope"],agentic:true,backWhat:"An essential article by Simon Willison (creator of Datasette) that describes the fundamental security threat model for AI agents: the combination of Prompt Injection + Tool Use + Permissions that creates the core vulnerability pattern in agentic AI systems.",backSecurity:"This is the conceptual framework every AI security professional should internalise. The \'Lethal Trifecta\' explains why agentic AI is fundamentally dangerous: LLMs that can be manipulated (prompt injection) given the ability to act (tool use) with real permissions creates an exploitable attack surface that no single control fully mitigates.",backWhen:"Read this as foundational education before designing security controls for any AI agent system. Share with engineering teams building agentic applications. Reference in threat models and architecture reviews. It takes 10 minutes to read and will reshape how you think about AI agent security."},
{name:"Identity Defined Security Alliance",url:"https://www.idsalliance.org/",category:"AI Governance & Standards",desc:"Non-profit framework for identity-centric security strategies.",riskRaw:"Safe",audience:"Blue Team",tags:["Identity","Framework","Standard"],llm:[],stages:["scope","govern"],agentic:false,backWhat:"A non-profit organisation providing frameworks and best practices for identity-centric security strategies. Publishes guidance on implementing identity-first security models across enterprise infrastructure.",backSecurity:"As AI agents proliferate, identity becomes the critical security control — every agent needs an identity, every API call needs authentication, every tool invocation needs authorisation. IDSA\'s frameworks for identity-centric security directly apply to securing AI agent permissions and workload identities.",backWhen:"Reference when designing identity architectures for AI systems, particularly when AI agents need to authenticate to services and APIs. The frameworks provide a structured approach to thinking about non-human identity in AI deployments."},
{name:"Itzik Alvas",url:"https://www.linkedin.com/in/itzik-alvas/",category:"AI Governance & Standards",desc:"Thought leader in AI Security. Follow for cutting-edge AI governance insights.",riskRaw:"Safe",audience:"All",tags:["Influencer","Thought Leader","LinkedIn"],llm:[],stages:[],agentic:false,backWhat:"A prominent thought leader in AI Security, co-founder of Entro Security. Publishes regularly on LinkedIn about non-human identity security, AI agent risks, secrets management, and the intersection of identity and AI security.",backSecurity:"Follow for timely insights on emerging AI security threats, particularly around non-human identity and secrets management in AI systems. Provides practical perspectives grounded in building security products, not just academic theory.",backWhen:"Follow on LinkedIn for ongoing AI security insights. Particularly valuable for staying current on non-human identity risks as AI agents proliferate across enterprise infrastructure."},
{name:"MITRE ATLAS",url:"https://atlas.mitre.org/",category:"AI Governance & Standards",desc:"Adversarial Threat Landscape for AI Systems. The MITRE ATT&CK equivalent for AI — foundational knowledge base.",riskRaw:"Safe",audience:"All",tags:["Threat Model","MITRE","Reference"],llm:["LLM01","LLM02","LLM03","LLM04","LLM05","LLM06","LLM07","LLM08","LLM09","LLM10"],stages:["scope"],agentic:false,backWhat:"The Adversarial Threat Landscape for AI Systems — MITRE\'s knowledge base of adversary tactics and techniques for attacking AI systems. The AI equivalent of MITRE ATT&CK, providing a structured taxonomy of how AI systems are attacked in the real world.",backSecurity:"ATLAS is foundational for AI threat modelling. It provides a common language for describing AI attacks, maps real-world case studies to techniques, and enables structured threat analysis. If ATT&CK is how you communicate about traditional threats, ATLAS is how you communicate about AI threats.",backWhen:"Use as a reference framework when conducting AI threat modelling, writing detection rules, or communicating AI risks to stakeholders. Essential knowledge base for any AI security programme. Browse it, learn the taxonomy, and reference it in assessments."},
{name:"CycloneDX (AIBOM)",url:"https://cyclonedx.org/",category:"AI Governance & Standards",desc:"SBOM standard extended for AI/ML Bill of Materials. Critical for AI supply chain governance.",riskRaw:"Safe",audience:"Builder",tags:["SBOM","Standard","Open Source"],llm:["LLM03","LLM05"],stages:["release","govern"],agentic:false,complexityOverride:"Expert Required",backWhat:"The CycloneDX standard extended for AI/ML Bill of Materials. Provides a structured format for documenting all components of an AI system — models, training data, hyperparameters, dependencies, and deployment configurations.",backSecurity:"AI supply chain attacks (LLM03, LLM05) are growing as organisations consume pre-trained models, fine-tuning datasets, and ML libraries from external sources. An AIBOM provides the inventory needed to track what\'s in your AI stack, detect compromised components, and respond to supply chain incidents.",backWhen:"Implement when you need formal supply chain governance for AI systems. Requires integration with CI/CD pipelines, ML training infrastructure, and model registries. Expert-level work that builds on existing SBOM practices."},
{name:"StrideGPT",url:"https://stridegpt.ai/",category:"AI Governance & Standards",desc:"AI-powered threat modeling tool. Uses LLMs to generate context-specific security requirements.",riskRaw:"Safe",audience:"Blue Team",tags:["Threat Model","Open Source","Tool"],llm:["LLM01","LLM02","LLM07"],stages:["scope"],agentic:false,backWhat:"An AI-powered threat modelling tool that uses LLMs to generate context-specific security requirements, threats, and mitigations. Applies the STRIDE methodology (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege) to AI systems.",backSecurity:"Accelerates threat modelling by using AI to generate initial threat assessments, attack trees, and mitigation recommendations. Particularly useful for security teams that need to threat model AI applications but lack deep AI security expertise — the tool provides a structured starting point.",backWhen:"Use at the design phase of AI projects to generate initial threat models. Web-based and straightforward to use. The output should be reviewed and refined by a security architect, but it dramatically reduces the time to a first-pass threat model."},
// === AI Guardrails & Firewalls (10) ===
{name:"LLM Guard (Protect AI)",url:"https://protectai.com/llm-guard",category:"AI Guardrails & Firewalls",desc:"Security scanner for LLM inputs and outputs — prevents injection and data leakage.",riskRaw:"Safe",audience:"Builder",tags:["Guardrails","Scanner","Open Source"],llm:["LLM01","LLM02","LLM06"],stages:["operate","deploy"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An open-source security toolkit from Protect AI that scans both LLM inputs and outputs in real-time. Provides modular scanners for prompt injection detection, PII redaction, toxicity filtering, and output validation.",backSecurity:"LLM Guard sits in the request/response path and applies security scanning at both ends. Input scanners catch injection attempts and sensitive data before it reaches the model. Output scanners detect PII leakage, toxic content, and malformed responses before they reach the user. Modular design means you deploy only the scanners you need.",backWhen:"Use when you need runtime input/output scanning for LLM applications. Python library that integrates into your application code — requires API integration and scanner configuration but well-documented. Good starting point for teams implementing guardrails for the first time."},
{name:"NeMo Guardrails (NVIDIA)",url:"https://developer.nvidia.com/nemo-guardrails",category:"AI Guardrails & Firewalls",desc:"Toolkit for adding programmable guardrails to LLM-based systems.",riskRaw:"Safe",audience:"Builder",tags:["Guardrails","NVIDIA","Open Source"],llm:["LLM01","LLM02"],stages:["operate","deploy"],agentic:false,complexityOverride:"Expert Required",backWhat:"NVIDIA\'s open-source toolkit for adding programmable guardrails to LLM-based systems. Uses Colang — a custom modelling language — to define conversational rails that control what the LLM can and cannot do.",backSecurity:"NeMo Guardrails provides deep, programmable control over LLM behaviour. Unlike pattern-matching filters, Colang rails can implement complex conversational policies: topic restrictions, fact-checking flows, and multi-turn safety checks. This granularity is essential for high-stakes applications.",backWhen:"Use when you need fine-grained control over LLM behaviour beyond simple input/output filtering. Requires learning Colang, designing rail policies, and integrating with your LLM pipeline. Expert-level work but provides the most customisable guardrail system available."},
{name:"LlamaFirewall (Meta)",url:"https://ai.meta.com/research/publications/llamafirewall-an-open-source-guardrail-system-for-building-secure-ai-agents/",category:"AI Guardrails & Firewalls",desc:"Host-level firewall for LLM agents to prevent malicious tool use.",riskRaw:"Safe",audience:"Builder",tags:["Firewall","Agents","Meta","Open Source"],llm:["LLM01","LLM07","LLM08"],stages:["operate","deploy"],agentic:true,complexityOverride:"Expert Required",backWhat:"Meta\'s open-source host-level firewall designed specifically for LLM agents. Prevents malicious tool use by intercepting and validating agent actions before they execute, acting as a security enforcement layer between the LLM and its tools.",backSecurity:"Directly addresses the Lethal Trifecta — LlamaFirewall intercepts tool calls from LLM agents and validates them against security policies before execution. This breaks the chain between prompt injection and harmful action by adding an independent validation layer that the LLM cannot bypass.",backWhen:"Deploy when building AI agents that use tools with real-world effects (file access, API calls, database queries). Requires architectural integration — the firewall must sit between the agent and its tool layer. Expert-level deployment but essential for any agent with meaningful permissions."},
{name:"E2B",url:"https://e2b.dev/",category:"AI Guardrails & Firewalls",desc:"Sandboxed code execution for AI agents. Prevents LLM-generated code from causing damage.",riskRaw:"Safe",audience:"Builder",tags:["Sandbox","Agents","Security"],llm:["LLM02","LLM07"],stages:["operate"],agentic:true,complexityOverride:"Guided Setup",backWhat:"A cloud-based sandboxed execution environment for AI-generated code. Provides isolated containers where LLM-generated code can run safely without access to the host system, network, or sensitive data.",backSecurity:"Code generation is one of the most powerful — and dangerous — capabilities of LLM agents. E2B ensures that when an agent generates and runs code, it executes in a fully isolated environment. This prevents malicious or buggy generated code from accessing sensitive data or compromising infrastructure.",backWhen:"Use when your AI agents generate and execute code. The SDK integrates with popular agent frameworks. Guided setup — requires API integration and environment configuration, but the sandboxing is managed for you."},
{name:"A2A Protocol",url:"https://a2a-protocol.org/latest/",category:"AI Guardrails & Firewalls",desc:"Agent-to-Agent protocol standards. Defines how AI agents communicate — critical for security boundaries.",riskRaw:"Safe",audience:"Builder",tags:["Protocol","Agents","Standard"],llm:["LLM07","LLM08"],stages:["scope","deploy"],agentic:true,complexityOverride:"Expert Required",backWhat:"The Agent-to-Agent protocol standard that defines how AI agents communicate, authenticate, and collaborate. Specifies the message formats, security boundaries, and trust models for multi-agent systems.",backSecurity:"As multi-agent systems proliferate, the communication layer between agents becomes a critical attack surface. A2A Protocol defines how agents establish trust, authenticate requests, and maintain security boundaries — preventing one compromised agent from manipulating others.",backWhen:"Implement when building multi-agent systems where agents need to communicate securely. Requires protocol implementation, identity management, and security boundary design. Expert-level architecture work but essential for any production multi-agent deployment."},
{name:"Pangea",url:"https://pangea.cloud/",category:"AI Guardrails & Firewalls",desc:"Developer-first AI security APIs: Sanitize, Redact, Auth, Data Guard, Prompt Guard. Security-as-a-service for AI apps.",riskRaw:"Safe",audience:"Builder",tags:["APIs","Developer","SaaS"],llm:["LLM01","LLM02","LLM06"],stages:["develop","deploy"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A developer-first platform providing security-as-a-service APIs for AI applications. Offers Prompt Guard (injection detection), AI Guard (content filtering), Redact (PII removal), Secure Audit Log, and authentication services — all as simple API calls.",backSecurity:"Pangea lowers the barrier to implementing AI security by providing production-ready security capabilities as APIs. Instead of building your own PII redaction or prompt injection detection, you make an API call. This is particularly valuable for development teams that need security controls but lack security engineering expertise.",backWhen:"Use when you want to add security capabilities to AI applications without building them from scratch. API-first approach means integration is straightforward — configure API keys, select services, integrate SDK calls. Days rather than weeks to deploy."},
{name:"Cisco AI Runtime",url:"https://www.cisco.com/site/us/en/solutions/ai/ai-defense/index.html",category:"AI Guardrails & Firewalls",desc:"Enterprise LLM firewall + guardrails. Prevents prompt injection, data loss, and compliance violations at scale.",riskRaw:"Safe",audience:"Blue Team",tags:["Firewall","Enterprise","Cisco"],llm:["LLM01","LLM02","LLM06","LLM07"],stages:["deploy","operate"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"Cisco\'s enterprise AI security solution providing LLM firewall capabilities, guardrails, and compliance enforcement at scale. Part of Cisco\'s broader AI Defense portfolio, it integrates with existing Cisco network and security infrastructure.",backSecurity:"Provides enterprise-grade AI security that integrates with the network layer — meaning it can enforce policies across all AI traffic traversing your infrastructure, not just specific applications. Covers prompt injection prevention, data loss prevention, and compliance policy enforcement.",backWhen:"Deploy when you need organisation-wide AI security enforcement integrated with existing Cisco infrastructure. Enterprise procurement, network integration, and policy management required. Best suited for organisations already in the Cisco ecosystem."},
{name:"F5 AI Gateway",url:"https://www.f5.com/products/ai-gateway",category:"AI Guardrails & Firewalls",desc:"Network-layer AI security gateway. Protects, accelerates, and observes AI-powered applications.",riskRaw:"Safe",audience:"Blue Team",tags:["Gateway","Network","F5"],llm:["LLM01","LLM02","LLM10"],stages:["operate"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"F5\'s network-layer AI security gateway that protects, accelerates, and observes AI-powered applications. Sits in the network path between users and AI services, providing security scanning, rate limiting, and observability.",backSecurity:"Network-layer enforcement means F5 AI Gateway can protect AI applications regardless of their implementation. It catches threats at the network boundary — prompt injection in HTTP payloads, data exfiltration in responses, and model theft through excessive API usage (LLM10).",backWhen:"Deploy when you need network-level AI security enforcement across multiple AI applications. Enterprise network appliance requiring procurement, infrastructure integration, and network architecture changes. Best suited for organisations already using F5 infrastructure."},
{name:"MCP Secure Gateway",url:"https://github.com/nicobailon/mcp-secure-gateway",category:"AI Guardrails & Firewalls",desc:"Runtime guardrails for MCP (Model Context Protocol) connections. Secures the emerging agent protocol standard.",riskRaw:"Safe",audience:"Builder",tags:["MCP","Open Source","Agents"],llm:["LLM01","LLM07","LLM08"],stages:["deploy","operate"],agentic:true,complexityOverride:"Expert Required",backWhat:"An open-source security gateway for Model Context Protocol (MCP) connections. Provides runtime guardrails, authentication, and policy enforcement for the protocol that connects LLMs to external tools and data sources.",backSecurity:"MCP is becoming the standard for connecting LLMs to tools — but the protocol itself has minimal built-in security. MCP Secure Gateway adds the missing security layer: authenticating MCP connections, validating tool calls against policies, and monitoring for suspicious patterns.",backWhen:"Deploy when using MCP-based agent architectures in production. Requires MCP protocol knowledge, deployment infrastructure, and security policy configuration. Essential for any production MCP deployment but requires expert-level understanding of the protocol."},
{name:"ZenGuard AI",url:"https://www.zenguard.ai/",category:"AI Guardrails & Firewalls",desc:"Dev-first API platform for lowest-latency GenAI guardrails and vulnerability testing.",riskRaw:"Safe",audience:"Builder",tags:["APIs","Guardrails","Platform"],llm:["LLM01","LLM02","LLM06"],stages:["test","operate"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A developer-first API platform providing low-latency GenAI guardrails and vulnerability testing. Focuses on delivering the fastest possible guardrail response times to minimise impact on user experience.",backSecurity:"Latency is the practical barrier to guardrail adoption — if your security scanning adds noticeable delay, developers will bypass it. ZenGuard AI optimises for speed, making it feasible to add guardrails to latency-sensitive applications like chatbots and real-time assistants.",backWhen:"Use when latency is a critical requirement for your guardrail deployment. API-based integration with SDK support. Guided setup similar to Pangea — configure API keys, select guardrails, integrate into your application."},
// === AI Development Tools (7) ===
{name:"LangChain",url:"https://www.langchain.com/",category:"AI Development Tools",desc:"LLM orchestration framework for building AI agents and chains. Core infrastructure every security team must understand.",riskRaw:"Safe",audience:"Builder",tags:["Framework","Agents","Orchestration"],llm:[],stages:["develop"],agentic:true,complexityOverride:"Expert Required",backWhat:"The dominant LLM orchestration framework for building AI agents and chains. Provides abstractions for connecting LLMs to tools, data sources, memory, and other components. Core infrastructure that a huge portion of AI applications are built on.",backSecurity:"Every security team needs to understand LangChain because a large percentage of the AI applications they\'ll assess are built with it. Understanding LangChain\'s chain architecture, tool binding mechanisms, and memory patterns is essential for identifying where security controls should be applied.",backWhen:"Study when you need to understand how AI applications are built — this is foundational knowledge for AI security assessments. Using LangChain to build requires framework-level commitment: architectural decisions, ongoing maintenance, and keeping up with rapid API changes."},
{name:"LlamaIndex",url:"https://www.llamaindex.ai/",category:"AI Development Tools",desc:"Data framework connecting LLMs to external sources. RAG pipeline backbone — key vector for data exfiltration and injection.",riskRaw:"Safe",audience:"Builder",tags:["RAG","Data","Framework"],llm:["LLM06"],stages:["develop"],agentic:false,complexityOverride:"Expert Required",backWhat:"A data framework for connecting LLMs to external data sources, primarily through Retrieval Augmented Generation (RAG) pipelines. Handles document ingestion, indexing, vector storage, and retrieval — the backbone of knowledge-grounded AI applications.",backSecurity:"RAG pipelines are a primary vector for data exfiltration (LLM06) and injection attacks. LlamaIndex manages the pipeline that retrieves context and feeds it to the LLM — meaning it controls what data the model sees. Poisoning the index, manipulating retrieval, or extracting sensitive chunks are all real attack vectors.",backWhen:"Study when assessing RAG-based AI applications. Using LlamaIndex to build requires data pipeline design, vector store selection, index configuration, and ongoing maintenance. Expert-level framework that underpins most enterprise RAG deployments."},
{name:"Langfuse",url:"https://langfuse.com/",category:"AI Development Tools",desc:"Open-source LLM observability — traces, evals, prompt management. Critical audit trail for AI systems.",riskRaw:"Safe",audience:"Builder",tags:["Observability","Tracing","Open Source"],llm:[],stages:["monitor","operate"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An open-source LLM observability platform providing traces, evaluations, and prompt management. Captures every LLM interaction — prompts, completions, latency, costs, and scores — creating a complete audit trail for AI systems.",backSecurity:"Observability is the foundation of AI security monitoring. Without traces, you can\'t detect prompt injection attempts, data leakage, or anomalous model behaviour. Langfuse provides the audit trail that security teams need to investigate incidents and demonstrate compliance.",backWhen:"Deploy when you need observability and audit trails for LLM applications. SDK integration is straightforward — add tracing calls to your application code. Self-hosted or cloud options. Essential infrastructure for any AI security monitoring programme."},
{name:"Arize Phoenix",url:"https://arize.com/phoenix/",category:"AI Development Tools",desc:"Open-source LLM tracing, evaluation, and hallucination detection platform.",riskRaw:"Safe",audience:"Builder",tags:["Observability","Evals","Open Source"],llm:["LLM09"],stages:["monitor"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An open-source LLM tracing, evaluation, and hallucination detection platform. Provides detailed traces of LLM interactions with built-in evaluation frameworks for measuring output quality, relevance, and factual accuracy.",backSecurity:"Hallucination detection is a security concern — LLM09 (Overreliance) becomes dangerous when models generate confident but incorrect information that users trust. Arize Phoenix\'s evaluation framework helps quantify hallucination rates and identify patterns in unreliable outputs.",backWhen:"Use when you need to measure and monitor LLM output quality, particularly hallucination rates. Python-based with straightforward integration. Guided setup — install, instrument your application, configure evaluations."},
{name:"OpenLIT",url:"https://openlit.io/",category:"AI Development Tools",desc:"OpenTelemetry-native LLM observability — one-line integration for cost, token, and prompt tracking.",riskRaw:"Safe",audience:"Builder",tags:["Observability","OTEL","Open Source"],llm:[],stages:["monitor"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An OpenTelemetry-native LLM observability solution that provides one-line integration for tracking costs, tokens, prompts, and performance metrics. Built on the OTEL standard for seamless integration with existing observability stacks.",backSecurity:"If your organisation already uses OpenTelemetry for application observability, OpenLIT extends that investment to AI workloads. Security teams can monitor AI metrics alongside application metrics in the same dashboards, making it easier to correlate AI anomalies with broader system events.",backWhen:"Use when you have an existing OTEL-based observability stack and want to add LLM monitoring. One-line integration is genuinely quick, but you need the OTEL infrastructure already in place. Natural fit for teams with mature observability practices."},
{name:"Scale AI",url:"https://scale.com/",category:"AI Development Tools",desc:"Data labeling and model evaluation services. Key infrastructure for AI security testing pipelines.",riskRaw:"Safe",audience:"Builder",tags:["Data","Evaluation","ML"],llm:[],stages:["test","augment"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"An enterprise platform for data labelling, model evaluation, and AI testing services. Provides human-in-the-loop evaluation, dataset curation, and benchmarking services used by major AI companies and government agencies.",backSecurity:"Scale AI\'s evaluation services are relevant for security testing at scale — particularly for measuring model safety properties, evaluating guardrail effectiveness, and benchmarking security controls. Their government and enterprise focus means they understand compliance requirements.",backWhen:"Engage when you need large-scale evaluation services backed by human reviewers. Enterprise procurement process with project scoping. This is a service engagement, not a software tool — think consulting plus platform."},
{name:"OTEL OpenLit Demo",url:"https://github.com/t00mas/otel-openlit-llm-metrics-demo",category:"AI Development Tools",desc:"OpenTelemetry demo for LLM metrics observability.",riskRaw:"Safe",audience:"Builder",tags:["Observability","Open Source","Metrics"],llm:[],stages:["monitor"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A demonstration repository showing how to implement OpenTelemetry-based LLM metrics observability using OpenLIT. Provides a working reference implementation with Docker Compose setup.",backSecurity:"Serves as a practical reference for teams wanting to implement LLM observability with OTEL. The demo shows exactly how to instrument AI applications, configure collectors, and visualise LLM metrics — reducing the guesswork of setting up observability from scratch.",backWhen:"Use as a starting point when implementing OTEL-based LLM observability. Clone the repo, run Docker Compose, and explore. Educational resource that accelerates your own implementation."},
// === AI Code Assistants (6) ===
{name:"GitHub Copilot",url:"https://github.com/features/copilot",category:"AI Code Assistants",desc:"AI coding assistant. Requires governance policy for proprietary code exposure.",riskRaw:"Medium Risk",audience:"Builder",tags:["Code","Dev","Microsoft"],llm:["LLM06"],stages:["develop"],agentic:false,complexityOverride:"Guided Setup",backWhat:"GitHub\'s AI coding assistant powered by OpenAI models. Provides code completion, chat-based development, and increasingly autonomous coding capabilities within VS Code, JetBrains, and other IDEs.",backSecurity:"The primary security concern is LLM06 (Sensitive Information Disclosure) — proprietary code, internal API patterns, and security-sensitive logic being sent to external AI models. Additionally, Copilot-generated code may contain vulnerabilities that developers accept without review. Governance policies are essential.",backWhen:"If your organisation uses Copilot, ensure governance policies cover data classification (what code can be sent to the model), code review requirements for AI-generated code, and enterprise configuration to limit data exposure. Requires org-level admin setup beyond individual installation."},
{name:"Cursor",url:"https://www.cursor.com/en",category:"AI Code Assistants",desc:"AI-native code editor. Same governance implications as Copilot for proprietary code.",riskRaw:"Medium Risk",audience:"Builder",tags:["Code","Dev","Editor"],llm:["LLM06"],stages:["develop"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An AI-native code editor built on VS Code with deep model integration. Provides code completion, multi-file editing, codebase-aware chat, and increasingly autonomous coding capabilities. Same underlying technology concerns as GitHub Copilot.",backSecurity:"Same governance implications as Copilot — proprietary code exposure to external models, generated code quality and security, and shadow AI risk if developers adopt it without approval. Cursor\'s codebase indexing means it may process more code context than simpler assistants.",backWhen:"Govern the same way as Copilot. If developers are using Cursor, ensure governance policies address code classification, model provider selection, and review requirements. Desktop application requiring installation, subscription, and governance policy compliance."},
{name:"Continue",url:"https://www.continue.dev/",category:"AI Code Assistants",desc:"Open-source AI code assistant for VS Code and JetBrains. Self-hostable for data control.",riskRaw:"Safe",audience:"Builder",tags:["Code","Open Source","IDE"],llm:["LLM06"],stages:["develop"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An open-source AI code assistant for VS Code and JetBrains. Connects to any LLM provider (including self-hosted models) and provides code completion, chat, and editing capabilities with full control over where your code is sent.",backSecurity:"Continue\'s key security advantage is model provider flexibility — you can point it at self-hosted models (via Ollama or similar) to keep all code on-premises. This makes it the strongest option for organisations with strict data sovereignty requirements where no code can leave the network.",backWhen:"Use when data sovereignty requires self-hosted models for code assistance. Requires IDE extension setup, model provider configuration, and potentially self-hosted model infrastructure. The guided setup varies significantly depending on whether you use cloud or self-hosted models."},
{name:"Tabnine",url:"https://www.tabnine.com/",category:"AI Code Assistants",desc:"AI code assistant with enterprise focus — private models, no code leaves your environment.",riskRaw:"Safe",audience:"Builder",tags:["Code","Enterprise","Private"],llm:["LLM06"],stages:["develop"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An AI code assistant with an enterprise focus, offering private models that can be trained on your codebase without data leaving your environment. Provides code completion and chat with strict data privacy guarantees.",backSecurity:"Tabnine\'s differentiator is the privacy-first architecture — enterprise versions train private models on your code without sending data to external providers. This addresses the core LLM06 concern that makes other code assistants risky for sensitive codebases.",backWhen:"Evaluate when you need AI code assistance but cannot send code to external providers. Enterprise configuration includes admin controls, model privacy settings, and compliance features. Guided setup for the IDE extension, but enterprise deployment requires admin configuration."},
{name:"Tabby",url:"https://www.tabbyml.com/",category:"AI Code Assistants",desc:"Self-hosted AI coding assistant. Open-source alternative for data sovereignty requirements.",riskRaw:"Safe",audience:"Builder",tags:["Code","Self-Hosted","Open Source"],llm:["LLM06"],stages:["develop"],agentic:false,complexityOverride:"Expert Required",backWhat:"A self-hosted open-source AI coding assistant. Runs entirely on your infrastructure with your choice of model, providing code completion and chat without any data leaving your environment.",backSecurity:"Complete data sovereignty — everything runs on your hardware with your chosen model. No external API calls, no data transmission, no third-party processing. The strongest privacy guarantee available for AI code assistance, but comes with the operational burden of self-hosting.",backWhen:"Deploy when you require air-gapped or fully sovereign AI code assistance. Requires GPU server provisioning, model selection and download, deployment configuration, and ongoing maintenance. Expert-level infrastructure work but the only option for truly air-gapped environments."},
{name:"SoftwareAnalyst.io",url:"https://softwareanalyst.io/",category:"AI Code Assistants",desc:"Automated software analysis tool. Verify code privacy policies before use.",riskRaw:"Medium",audience:"Builder",tags:["Code Analysis","Dev","Automation"],llm:[],stages:["develop"],agentic:false,backWhat:"A web-based automated software analysis tool that provides code review, architecture analysis, and security scanning capabilities for software projects.",backSecurity:"Verify the tool\'s code privacy policies before uploading proprietary code. Web-based analysis tools process code externally, which may conflict with data classification requirements. Understand what data is retained and how it\'s used.",backWhen:"Use for initial code analysis when the privacy implications are acceptable. Web-based with minimal setup. Always review the data handling policies before uploading sensitive or proprietary code."},
// === Foundation Models (6) ===
{name:"Ollama",url:"https://ollama.com/",category:"Foundation Models",desc:"Run LLMs locally. Essential for air-gapped deployments, sovereign AI, and red team labs.",riskRaw:"Safe",audience:"Builder",tags:["Local","Open Source","Self-Hosted"],llm:[],stages:["deploy"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A tool for running LLMs locally on your own hardware. Downloads, manages, and serves open-source models with a simple CLI. Supports a wide range of models from Llama to Mistral to custom fine-tunes.",backSecurity:"Essential infrastructure for AI security work. Running models locally means no data leaves your environment — critical for red team labs, sensitive testing, and air-gapped deployments. Also enables testing against specific model versions without API variability.",backWhen:"Use when you need local LLM inference for testing, red teaming, or sovereign deployment. Requires local installation, sufficient hardware (RAM/GPU depending on model size), and CLI familiarity. Guided setup — the tool is straightforward but choosing the right model and hardware configuration requires some knowledge."},
{name:"DeepSeek",url:"https://www.deepseek.com/en",category:"Foundation Models",desc:"Chinese LLM. Strong at coding but presents significant data privacy and sovereignty risks.",riskRaw:"Critical (Red Flag)",audience:"Builder",tags:["LLM","Code","China","Privacy Risk"],llm:["LLM06"],stages:[],agentic:false,complexityOverride:"Expert Required",backWhat:"A Chinese AI company producing high-performance LLMs, particularly strong at code generation. DeepSeek models have achieved competitive benchmarks at lower cost, making them attractive but controversial due to data sovereignty concerns.",backSecurity:"DeepSeek presents significant data privacy and sovereignty risks. As a Chinese-headquartered company, data processed through DeepSeek\'s API may be subject to Chinese data access laws. The models themselves can be run locally via Ollama to mitigate API risks, but the governance decision requires careful analysis of your regulatory environment.",backWhen:"Evaluate with extreme caution in regulated environments. If using DeepSeek models, run them locally rather than through the API. Requires a thorough governance review covering data sovereignty, regulatory compliance, and supply chain risk before any organisational adoption."},
{name:"Mistral AI",url:"https://mistral.ai/",category:"Foundation Models",desc:"European foundation models and agents. EU data sovereignty alternative to US providers.",riskRaw:"Safe",audience:"Builder",tags:["Foundation Model","EU","Open Source"],llm:[],stages:[],agentic:false,complexityOverride:"Guided Setup",backWhat:"A European AI company building foundation models and agent capabilities. Offers both API access and open-weight models, positioning as the EU data sovereignty alternative to US AI providers.",backSecurity:"Mistral\'s European headquarters and EU data processing provide a sovereignty alternative for organisations concerned about US cloud provider data access. Their open-weight models can also be self-hosted for complete data control.",backWhen:"Evaluate when EU data sovereignty is a requirement. API access is straightforward (Guided Setup), but self-hosted deployment of larger models requires GPU infrastructure. The sovereignty story is Mistral\'s key differentiator for security-conscious European organisations."},
{name:"Hugging Face",url:"https://huggingface.co/",category:"Foundation Models",desc:"The ML community hub. Key attack surface for model supply chain — poisoned models, malicious datasets.",riskRaw:"Safe",audience:"Builder",tags:["Models","Open Source","ML"],llm:["LLM03","LLM05"],stages:["augment","develop"],agentic:false,complexityOverride:"Guided Setup",backWhat:"The largest open-source ML community hub hosting models, datasets, and spaces. Provides model discovery, evaluation, and deployment tools. The primary distribution channel for open-source AI models.",backSecurity:"Hugging Face is the biggest AI supply chain attack surface. Poisoned models, malicious datasets, and compromised model files are real threats (LLM03, LLM05). Anyone can upload models, and verification mechanisms are still maturing. Security teams need to understand the risks of pulling models from Hugging Face.",backWhen:"Use for model discovery and evaluation, but implement supply chain verification before deploying any model from Hugging Face. Check model provenance, scan for malicious code in model files, and validate model behaviour before production use."},
{name:"Llama (Meta)",url:"https://www.llama.com/",category:"Foundation Models",desc:"Open-source foundation models. Red teams need to understand capabilities for adversarial testing.",riskRaw:"Safe",audience:"Builder",tags:["Foundation Model","Open Source","Meta"],llm:[],stages:[],agentic:false,complexityOverride:"Expert Required",backWhat:"Meta\'s family of open-source foundation models. Available in multiple sizes (from 1B to 405B parameters) for self-hosted deployment. The most widely used open-source model family for enterprise AI.",backSecurity:"Self-hosted Llama models provide data sovereignty — no data leaves your environment. However, self-hosting means you\'re responsible for the full security stack: model integrity, serving infrastructure, access controls, and monitoring. Understanding Llama\'s capabilities and limitations is essential for red team planning.",backWhen:"Deploy when you need sovereign, self-hosted foundation model capabilities. Requires GPU infrastructure, model serving setup (via Ollama, vLLM, or similar), and operational expertise. Expert-level deployment but the most mature option for enterprise self-hosted AI."},
{name:"Perplexity",url:"https://www.perplexity.ai",category:"Foundation Models",desc:"Conversational web search using multiple LLMs. Enterprise shadow AI risk vector.",riskRaw:"Medium Risk",audience:"All",tags:["LLM","Search","Shadow AI"],llm:["LLM06"],stages:[],agentic:false,backWhat:"A conversational web search engine powered by multiple LLMs. Provides sourced, cited answers to queries by searching the web and synthesising results in real-time.",backSecurity:"Perplexity is a shadow AI risk vector — employees using it for research may inadvertently share sensitive query context with external AI services. Unlike traditional search engines, Perplexity processes and reasons about query context, potentially exposing more information than a simple search query would.",backWhen:"Monitor for shadow AI usage in your organisation. Perplexity itself is a browser-based tool with zero setup — the security concern is governance and awareness rather than technical controls."},
// === Identity & AppSec (10) ===
{name:"GitGuardian",url:"https://www.gitguardian.com/",category:"Identity & AppSec",desc:"Real-time secrets detection. Critical for preventing AI API key leaks in code repositories.",riskRaw:"Safe",audience:"Blue Team",tags:["Secrets","DLP","DevSecOps"],llm:["LLM06"],stages:["develop","monitor"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A real-time secrets detection platform that scans code repositories, CI/CD pipelines, and collaboration tools for exposed credentials. Detects API keys, tokens, certificates, and other secrets before they become security incidents.",backSecurity:"AI deployments multiply the secrets attack surface — API keys for LLM providers, vector database credentials, model registry tokens, and service account keys all need protection. GitGuardian catches these before they\'re committed to repositories or shared in CI/CD logs.",backWhen:"Deploy across all repositories that contain AI application code. SaaS with CI/CD integration requiring repo connections, policy configuration, and alert tuning. Critical infrastructure for any development team working with AI APIs and services."},
{name:"Noma Security",url:"https://noma.security/",category:"Identity & AppSec",desc:"End-to-end AI application security platform. Covers the full AI development lifecycle with posture management.",riskRaw:"Safe",audience:"Blue Team",tags:["AppSec","Lifecycle","AI-SPM"],llm:["LLM01","LLM02","LLM03","LLM04","LLM05","LLM06","LLM07","LLM08","LLM09","LLM10"],stages:["scope","develop","test","deploy","operate","monitor","govern"],agentic:true,complexityOverride:"Enterprise Only",backWhat:"An end-to-end AI application security platform providing AI Security Posture Management (AI-SPM). Covers the full AI development lifecycle from data preparation through production monitoring, with visibility into model risks, data flows, and security posture.",backSecurity:"Noma provides the broadest coverage of any AI security platform — spanning all 10 OWASP LLM risks across all lifecycle stages. Their AI-SPM approach gives security teams a single pane of glass for understanding AI risk across the organisation.",backWhen:"Evaluate when you need comprehensive AI security coverage from a single platform. Enterprise procurement with org-wide deployment across AI development teams and infrastructure. This is a platform commitment, not a point tool."},
{name:"Aembit",url:"https://aembit.io/",category:"Identity & AppSec",desc:"Workload Identity and Access Management — securing non-human identities in AI pipelines.",riskRaw:"Safe",audience:"Blue Team",tags:["Identity","IAM","Workload"],llm:["LLM08"],stages:["deploy"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"A Workload Identity and Access Management platform that secures non-human identities in AI pipelines. Provides secretless authentication between workloads, eliminating the need for static credentials in AI infrastructure.",backSecurity:"AI pipelines create dense webs of service-to-service communication — model serving, data stores, vector databases, monitoring tools. Each connection traditionally requires credentials. Aembit replaces static secrets with dynamic workload identity, reducing the credential attack surface to zero.",backWhen:"Deploy when securing the identity layer of AI infrastructure. Enterprise platform requiring procurement, infrastructure integration, and migration from static credentials. Most valuable in complex AI deployments with many service-to-service connections."},
{name:"Entro Security",url:"https://entro.security/",category:"Identity & AppSec",desc:"Non-Human Identity Management. Discovers and secures machine identities and secrets.",riskRaw:"Safe",audience:"Blue Team",tags:["Identity","NHI","Secrets"],llm:["LLM08"],stages:["monitor","operate"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"A Non-Human Identity Management platform that discovers and secures machine identities and secrets across enterprise infrastructure. Provides visibility into the full lifecycle of non-human credentials — from creation to rotation to decommissioning.",backSecurity:"AI systems generate a proliferation of non-human identities — API keys, service accounts, OAuth tokens, certificates — that are often unmanaged and over-privileged. Entro discovers these identities, assesses their risk, and provides lifecycle management to prevent credential-based attacks.",backWhen:"Deploy when you need visibility and governance over the non-human identities in your AI infrastructure. Enterprise platform requiring procurement, infrastructure discovery, and integration with identity providers and secret managers."},
{name:"SPIFFE",url:"https://spiffe.io/",category:"Identity & AppSec",desc:"Secure Production Identity Framework — the open standard for workload identity in AI infrastructure.",riskRaw:"Safe",audience:"Builder",tags:["Identity","Standard","Open Source"],llm:["LLM08"],stages:["deploy"],agentic:false,complexityOverride:"Expert Required",backWhat:"The Secure Production Identity Framework for Everyone — an open standard for workload identity in cloud-native and AI infrastructure. SPIRE (the SPIFFE Runtime Environment) provides the production implementation for issuing and managing workload identities.",backSecurity:"SPIFFE provides cryptographic workload identity without static credentials. Every workload — including AI model serving instances, data pipelines, and agent processes — gets a verifiable identity (SVID) that enables mutual TLS and fine-grained authorisation. This is the foundation for zero-trust AI infrastructure.",backWhen:"Implement when building zero-trust infrastructure for AI workloads. Requires deploying SPIRE servers, registering workloads, configuring PKI, and integrating with your service mesh. Expert-level infrastructure work but the open-standard approach avoids vendor lock-in."},
{name:"Silverfort",url:"https://www.silverfort.com/",category:"Identity & AppSec",desc:"Unified Identity Protection platform with MFA for service accounts and AI workloads.",riskRaw:"Safe",audience:"Blue Team",tags:["Identity","MFA","IAM"],llm:["LLM08"],stages:["deploy","operate"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"A Unified Identity Protection platform that extends MFA and adaptive authentication to resources that traditionally can\'t support it — including service accounts, legacy applications, and AI workloads.",backSecurity:"Service accounts and machine identities used by AI systems are often the weakest link — they can\'t do MFA, they have static passwords, and they\'re over-privileged. Silverfort applies adaptive authentication and risk-based access controls to these identities without modifying the underlying systems.",backWhen:"Deploy when you need to secure service accounts and machine identities used by AI infrastructure. Enterprise platform requiring procurement, Active Directory integration, and organisation-wide policy deployment."},
{name:"Wiz",url:"https://www.wiz.io/",category:"Identity & AppSec",desc:"Cloud security platform with AI-SPM features for securing AI workloads in cloud.",riskRaw:"Safe",audience:"Blue Team",tags:["Cloud","CNAPP","AI-SPM"],llm:["LLM04","LLM08"],stages:["scope","monitor"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"The leading Cloud Native Application Protection Platform (CNAPP) with AI Security Posture Management (AI-SPM) capabilities. Provides agentless cloud security with specific features for discovering and securing AI workloads in cloud environments.",backSecurity:"Wiz\'s AI-SPM features discover AI workloads across your cloud environment — model training jobs, serving endpoints, data pipelines — and assess their security posture. This gives security teams visibility into AI infrastructure they may not know exists, including shadow AI deployments.",backWhen:"Deploy when you need cloud security coverage that includes AI workload discovery and assessment. Enterprise CNAPP requiring procurement, cloud account integration, and organisation-wide deployment. Most valuable for multi-cloud environments with distributed AI workloads."},
{name:"AppOmni",url:"https://appomni.com/",category:"Identity & AppSec",desc:"SaaS Security Posture Management — visibility into AI SaaS configurations and data flows.",riskRaw:"Safe",audience:"Blue Team",tags:["SSPM","SaaS","Security"],llm:["LLM06"],stages:["monitor"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"A SaaS Security Posture Management (SSPM) platform providing visibility into AI-related SaaS configurations and data flows. Monitors how AI features in SaaS applications are configured and what data they can access.",backSecurity:"As SaaS vendors embed AI features into their products, new data exposure risks emerge. AppOmni monitors these AI configurations — are AI features enabled? What data can they access? Are data sharing settings appropriate? This catches misconfigurations that traditional CASB tools miss.",backWhen:"Deploy when you need visibility into AI features across your SaaS portfolio. Enterprise SSPM requiring procurement, SaaS integrations, and ongoing configuration monitoring. Most valuable for organisations with large SaaS estates adopting AI features."},
{name:"SpiceDB",url:"https://authzed.com/spicedb",category:"Identity & AppSec",desc:"Google Zanzibar-inspired fine-grained authorization database. 5.3k GitHub stars. Essential for AI app permissions.",riskRaw:"Safe",audience:"Builder",tags:["AuthZ","Open Source","Permissions"],llm:["LLM01","LLM07","LLM08"],stages:["scope","develop"],agentic:true,complexityOverride:"Expert Required",backWhat:"A Google Zanzibar-inspired fine-grained authorisation database. Provides relationship-based access control (ReBAC) that scales to billions of relationships, with schema versioning and consistency guarantees. 5.3k+ GitHub stars.",backSecurity:"AI applications need authorisation that traditional RBAC can\'t provide — per-document access in RAG pipelines, per-tool permissions for agents, and context-dependent access decisions. SpiceDB\'s relationship-based model maps naturally to these AI-specific authorisation patterns.",backWhen:"Implement when you need fine-grained authorisation for AI applications, particularly RAG systems with document-level access control or agent systems with tool-level permissions. Expert-level work requiring schema design, deployment, and application integration."},
{name:"Teleport",url:"https://goteleport.com/",category:"Identity & AppSec",desc:"Infrastructure access platform extending to AI/LLM workloads. Zero-trust for AI infrastructure.",riskRaw:"Safe",audience:"Blue Team",tags:["Zero-Trust","Infrastructure","Open Source"],llm:["LLM08"],stages:["deploy","monitor","govern"],agentic:false,complexityOverride:"Expert Required",backWhat:"An infrastructure access platform providing zero-trust access to servers, Kubernetes clusters, databases, and AI/ML workloads. Uses short-lived certificates instead of static keys, with session recording and audit logging.",backSecurity:"Provides zero-trust access to the infrastructure that AI systems run on — GPU clusters, model serving platforms, training environments, and data stores. Session recording creates audit trails for all administrative access to AI infrastructure, essential for compliance and incident investigation.",backWhen:"Deploy when you need zero-trust infrastructure access for AI workloads. Requires deployment, certificate management, and integration with your infrastructure. Expert-level work but provides strong access governance for AI infrastructure."},
// === Third-Party Risk (19) ===
{name:"BitSight",url:"https://www.bitsight.com/",category:"Third-Party Risk",desc:"AI-powered cyber risk intelligence across attack surface and third-party ecosystem.",riskRaw:"Safe",audience:"Blue Team",tags:["Ratings","Platform","TPRM"],llm:[],stages:["scope","monitor"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"An AI-powered cyber risk intelligence platform providing security ratings, attack surface management, and continuous monitoring across your third-party ecosystem. Uses data-driven models to quantify cyber risk.",backSecurity:"BitSight\'s AI-powered risk quantification is increasingly relevant as organisations assess AI vendors. Understanding a vendor\'s security posture before giving them access to training data or model endpoints is critical for AI supply chain governance.",backWhen:"Deploy when you need continuous third-party risk monitoring at enterprise scale. Procurement process with vendor portfolio integration. Most valuable for organisations managing large vendor ecosystems that include AI service providers."},
{name:"Conveyor",url:"https://www.conveyor.com/",category:"Third-Party Risk",desc:"AI Agents for Trust Center automation, security questionnaires and RFx response.",riskRaw:"Safe",audience:"Blue Team",tags:["Trust Center","Automation","AI Agent"],llm:[],stages:["scope"],agentic:true,complexityOverride:"Guided Setup",backWhat:"A trust centre platform with AI-powered automation for security questionnaires and RFx responses. Provides a public-facing trust centre plus AI agents that assist with questionnaire completion.",backSecurity:"Helps organisations demonstrate their AI security posture to customers and partners through a managed trust centre. The AI-assisted questionnaire completion accelerates response times for security assessments — relevant as more customers ask AI-specific security questions.",backWhen:"Use when you need a customer-facing trust centre and want to accelerate security questionnaire responses. SaaS with content upload, questionnaire configuration, and branding setup. Guided setup that a security operations team can manage."},
{name:"Drata",url:"https://drata.com/",category:"Third-Party Risk",desc:"Trust centre with Compliance as Code and AI Questionnaire Assistance. Acquired SafeBase.",riskRaw:"Safe",audience:"Blue Team",tags:["Compliance","Automation","SaaS"],llm:[],stages:["govern"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"An enterprise compliance automation platform providing continuous evidence collection, trust centres, and AI-assisted questionnaire completion. Acquired SafeBase to expand trust centre capabilities. Supports SOC2, ISO 27001, and emerging AI governance frameworks.",backSecurity:"Drata\'s compliance automation is increasingly relevant as AI governance frameworks (ISO 42001, EU AI Act) require auditable evidence. Automated evidence collection reduces the manual burden of demonstrating AI compliance, while the trust centre communicates your AI security posture to customers.",backWhen:"Deploy when you need automated compliance evidence collection and trust centre capabilities. Enterprise platform requiring procurement, integration with 100+ tools, and organisation-wide adoption. Most valuable when pursuing AI-specific certifications."},
{name:"Loopio",url:"https://loopio.com/",category:"Third-Party Risk",desc:"AI-powered RFP software for procurement and TPRM workflows.",riskRaw:"Safe",audience:"Blue Team",tags:["RFP","Procurement","Automation"],llm:[],stages:["scope"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An AI-powered RFP and security questionnaire software platform. Uses machine learning to suggest answers from a content library, accelerating procurement and TPRM workflows.",backSecurity:"As AI security questions become standard in procurement questionnaires, having AI-powered response capabilities becomes operationally important. Loopio helps teams build and maintain a knowledge base of AI security answers that can be reused across questionnaires.",backWhen:"Use when your team regularly responds to security questionnaires and RFPs. SaaS requiring content library setup, team onboarding, and integration with procurement workflows."},
{name:"Mitratech Prevalent",url:"https://mitratech.com/products/prevalent/",category:"Third-Party Risk",desc:"End-to-end TPRM for every stage of the vendor lifecycle.",riskRaw:"Safe",audience:"Blue Team",tags:["TPRM","Lifecycle","Platform"],llm:[],stages:["scope","monitor","govern"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"An end-to-end Third-Party Risk Management platform covering every stage of the vendor lifecycle. Provides risk assessments, compliance monitoring, and remediation workflows for managing vendor relationships.",backSecurity:"Prevalent\'s lifecycle approach is relevant as AI vendor relationships require ongoing monitoring — not just initial assessment. Model updates, data handling changes, and new AI features all affect risk posture and need continuous evaluation.",backWhen:"Deploy when you need comprehensive TPRM coverage across the full vendor lifecycle. Enterprise platform requiring procurement, vendor portfolio migration, and process redesign."},
{name:"Panorays",url:"https://panorays.com/",category:"Third-Party Risk",desc:"Security questionnaires with task automations and auto-fill response tools.",riskRaw:"Safe",audience:"Blue Team",tags:["Questionnaire","Automation","Platform"],llm:[],stages:["scope"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A TPRM platform focused on security questionnaires with AI-powered task automation and auto-fill response capabilities. Streamlines the questionnaire exchange process between organisations and their vendors.",backSecurity:"Accelerates the vendor assessment process with AI automation, freeing security teams to focus on evaluating AI-specific risks rather than administrative questionnaire processing. The automation also ensures more consistent assessments across vendors.",backWhen:"Use when you need to streamline vendor security assessments. SaaS with vendor list upload and workflow configuration. Guided setup that a TPRM team can implement."},
{name:"ProcessUnity",url:"https://www.processunity.com/",category:"Third-Party Risk",desc:"Global Risk Exchange with NLP and GenAI for automated controls review. 360K vendor profiles.",riskRaw:"Safe",audience:"Blue Team",tags:["TPRM","NLP","AI Agent"],llm:[],stages:["scope","monitor"],agentic:true,complexityOverride:"Enterprise Only",backWhat:"An enterprise TPRM platform centred on a Global Risk Exchange with NLP and GenAI-powered controls review. Provides access to 360K+ vendor profiles and automated assessment capabilities.",backSecurity:"The vendor profile database and AI-powered controls review is relevant for assessing AI vendors at scale. NLP-based analysis of vendor documentation can identify security gaps faster than manual review, particularly when evaluating new AI service providers.",backWhen:"Deploy when you need enterprise-grade TPRM with AI-enhanced analysis. Procurement, vendor exchange configuration, and workflow design required."},
{name:"Responsive",url:"https://www.responsive.io/",category:"Third-Party Risk",desc:"RFP software leader with AI-driven response management.",riskRaw:"Safe",audience:"Blue Team",tags:["RFP","Procurement","AI"],llm:[],stages:["scope"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An AI-driven response management platform that leads the RFP software market. Uses AI to suggest and improve responses to security questionnaires, RFPs, and other procurement documents.",backSecurity:"Helps security teams respond efficiently to the growing volume of AI-related security questions in procurement processes. The AI-powered response suggestions ensure consistency across questionnaire responses about AI security practices.",backWhen:"Use when your team regularly responds to RFPs and security questionnaires. SaaS with content library setup and team configuration."},
{name:"RiskRecon",url:"https://www.riskrecon.com/",category:"Third-Party Risk",desc:"Cybersecurity ratings integrating with Whistic for AI-assisted questionnaire completion.",riskRaw:"Safe",audience:"Blue Team",tags:["Ratings","TPRM","Platform"],llm:[],stages:["scope","monitor"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"A Mastercard company providing cybersecurity risk ratings and continuous monitoring. Integrates with Whistic for AI-assisted questionnaire completion, combining outside-in ratings with vendor-provided assessment data.",backSecurity:"The integration of security ratings with questionnaire data provides a more complete picture of vendor risk. For AI vendors specifically, outside-in monitoring can detect infrastructure changes, certificate issues, or security degradation that questionnaire responses won\'t reveal.",backWhen:"Deploy when you need continuous outside-in monitoring of vendor security posture. Enterprise platform requiring procurement and vendor portfolio integration."},
{name:"SAFE",url:"https://safe.security/",category:"Third-Party Risk",desc:"AI-driven cyber risk quantification and management platform for TPRM.",riskRaw:"Safe",audience:"Blue Team",tags:["Risk Quantification","TPRM","Platform"],llm:[],stages:["scope"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"An AI-driven cyber risk quantification and management platform. Translates technical security metrics into financial risk language, enabling risk-based decision making for TPRM and AI security investments.",backSecurity:"Risk quantification in financial terms is essential for justifying AI security investments to leadership. SAFE translates technical findings (number of AI vulnerabilities, guardrail coverage gaps) into monetary risk exposure that CISOs can present to boards.",backWhen:"Deploy when you need to quantify AI security risk in financial terms for leadership communication. Enterprise platform requiring procurement and integration with risk data sources."},
{name:"SafeBase",url:"https://safebase.io/",category:"Third-Party Risk",desc:"Trust Center + AI Questionnaire Assistance.",riskRaw:"Safe",audience:"Blue Team",tags:["Trust Center","Questionnaire","SaaS"],llm:[],stages:["scope"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A trust centre platform (now part of Drata) that provides a public-facing portal for sharing security documentation, certifications, and compliance status with customers and partners.",backSecurity:"As customers increasingly ask about AI security practices, a trust centre provides a scalable way to communicate your AI governance posture. Self-service access to AI-related security documentation reduces the volume of ad-hoc security questionnaires.",backWhen:"Use when you need a customer-facing portal for AI security documentation. SaaS with content upload, branding, and access management setup."},
{name:"SecurityPal",url:"https://www.securitypalhq.com/",category:"Third-Party Risk",desc:"100x faster security reviews powered by AI Agents and expert humans.",riskRaw:"Safe",audience:"Blue Team",tags:["Reviews","AI Agent","Automation"],llm:[],stages:["scope"],agentic:true,complexityOverride:"Guided Setup",backWhat:"A platform combining AI agents with human security experts to deliver faster security reviews. The AI handles routine questionnaire completion while humans review complex or sensitive responses.",backSecurity:"The hybrid AI-plus-human approach is particularly relevant for AI security questionnaires, where some questions have standard answers (encryption in transit, SOC2 compliance) while others require nuanced, context-specific responses about AI governance and model security.",backWhen:"Use when you need to scale security review capacity without sacrificing quality on complex AI security questions. SaaS with content upload and workflow configuration."},
{name:"SecurityScorecard",url:"https://securityscorecard.com/",category:"Third-Party Risk",desc:"Supply Chain Detection and Response (SCDR) connecting TPRM and SOC teams.",riskRaw:"Safe",audience:"Blue Team",tags:["SCDR","Ratings","Platform"],llm:[],stages:["scope","monitor"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"A Supply Chain Detection and Response (SCDR) platform that connects TPRM and SOC teams. Provides security ratings, continuous monitoring, and incident response workflows for third-party risk.",backSecurity:"SecurityScorecard\'s SCDR approach is relevant for AI supply chain incidents — when an AI vendor is breached or a model supply chain is compromised, you need to quickly assess impact across your vendor portfolio and coordinate response.",backWhen:"Deploy when you need enterprise TPRM with SOC integration for incident response. Enterprise platform requiring procurement and vendor portfolio integration."},
{name:"TrustCloud",url:"https://www.trustcloud.ai/",category:"Third-Party Risk",desc:"Automate Security, Privacy, and AI Risk Assessments.",riskRaw:"Safe",audience:"Blue Team",tags:["Automation","Privacy","Risk"],llm:[],stages:["scope"],agentic:false,complexityOverride:"Guided Setup",backWhat:"An AI-powered platform for automating security, privacy, and risk assessments. Streamlines the evidence collection and assessment process for compliance frameworks.",backSecurity:"TrustCloud\'s automation is relevant as AI governance frameworks add new assessment requirements. Automating evidence collection for AI-specific controls reduces the compliance burden as regulatory requirements expand.",backWhen:"Use when you need automated risk and compliance assessment capabilities. SaaS with integration setup and assessment configuration."},
{name:"UpGuard",url:"https://www.upguard.com/",category:"Third-Party Risk",desc:"Security questionnaire automation, attack surface management, AI-powered Security Profiles.",riskRaw:"Safe",audience:"Blue Team",tags:["ASM","Questionnaire","Platform"],llm:[],stages:["scope","monitor"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A platform combining security questionnaire automation, attack surface management, and AI-powered Security Profiles. Provides both outside-in monitoring and vendor-provided assessment data.",backSecurity:"The combination of ASM and questionnaire automation provides visibility into AI vendors from both external scanning and self-reported assessment data. AI-powered Security Profiles automatically summarise vendor security posture for faster decision-making.",backWhen:"Use when you need both attack surface monitoring and questionnaire automation for vendor assessment. SaaS with vendor list setup, monitoring configuration, and questionnaire workflows."},
{name:"Vanta",url:"https://www.vanta.com/",category:"Third-Party Risk",desc:"Security compliance automation for SOC2, ISO 27001, and AI governance frameworks.",riskRaw:"Safe",audience:"Blue Team",tags:["Compliance","Automation","SaaS"],llm:[],stages:["govern"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"A security compliance automation platform supporting SOC2, ISO 27001, HIPAA, and emerging AI governance frameworks. Provides automated evidence collection with 300+ integrations, trust centres, and vendor risk management.",backSecurity:"Vanta\'s extensive integration library makes it well-suited for demonstrating AI governance compliance — connecting to cloud providers, code repositories, and infrastructure to automatically collect evidence for AI-specific controls.",backWhen:"Deploy when you need comprehensive compliance automation with AI governance support. Enterprise platform requiring procurement, integration configuration, and organisation-wide adoption."},
{name:"Whistic",url:"https://www.whistic.com/",category:"Third-Party Risk",desc:"Modernize TPRM with automated assessments, vendor insights, and continuous risk monitoring.",riskRaw:"Safe",audience:"Blue Team",tags:["TPRM","Vendor Risk","Platform"],llm:[],stages:["scope","monitor"],agentic:false,complexityOverride:"Guided Setup",backWhat:"A TPRM platform focused on modernising vendor risk assessments through automated questionnaires, vendor insights network, and continuous monitoring. Integrates with RiskRecon for combined ratings and assessment data.",backSecurity:"Whistic\'s vendor network approach is relevant for AI vendor assessment — if an AI vendor already has a Whistic profile, you can access pre-completed assessment data rather than starting from scratch, significantly accelerating the evaluation process.",backWhen:"Use when you need to streamline vendor security assessments with network-based data sharing. SaaS with vendor profile setup, assessment configuration, and monitoring."},
{name:"Zip",url:"https://ziphq.com/",category:"Third-Party Risk",desc:"Agentic procurement orchestration platform — AI-native intake to PO.",riskRaw:"Safe",audience:"Blue Team",tags:["Procurement","AI Agent","Enterprise"],llm:[],stages:["scope"],agentic:true,complexityOverride:"Enterprise Only",backWhat:"An agentic procurement orchestration platform providing AI-native intake-to-PO workflows. Uses AI agents to automate procurement processes from initial request through purchase order.",backSecurity:"Zip represents the emerging category of AI-native procurement — where AI agents handle parts of the procurement workflow. This creates both opportunity (faster vendor assessment) and risk (AI agents making procurement decisions need governance controls).",backWhen:"Deploy when you need AI-powered procurement automation. Enterprise platform requiring procurement process redesign and organisation-wide rollout."},
// === Compliance Automation (3) ===
{name:"Sprinto",url:"https://sprinto.com",category:"Compliance Automation",desc:"Compliance automation platform. Vanta competitor with strong integration coverage.",riskRaw:"Safe",audience:"Blue Team",tags:["Compliance","Automation","SaaS"],llm:[],stages:["govern"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"A compliance automation platform and strong Vanta competitor with extensive integration coverage. Automates evidence collection, control monitoring, and audit preparation across multiple compliance frameworks.",backSecurity:"Sprinto\'s integration-first approach is valuable as AI governance compliance requires connecting to diverse infrastructure — cloud providers, ML platforms, model registries, and monitoring tools. Automated evidence collection reduces the manual burden of demonstrating AI compliance.",backWhen:"Deploy when you need compliance automation with strong integration coverage. Enterprise platform requiring procurement, organisation-wide integration, and ongoing compliance monitoring."},
{name:"Delve",url:"https://delve.co",category:"Compliance Automation",desc:"Compliance automation via AI agent that integrates directly into your tech stack for evidence gathering.",riskRaw:"Safe",audience:"Blue Team",tags:["Compliance","AI Agent","Automation"],llm:[],stages:["govern"],agentic:true,complexityOverride:"Guided Setup",backWhat:"A compliance automation platform that uses AI agents to integrate directly into your tech stack for evidence gathering. The AI-first approach means evidence collection is more automated and less dependent on manual processes.",backSecurity:"Delve\'s AI agent approach to evidence collection is particularly relevant for AI governance — the agent can discover and document AI workloads, model configurations, and security controls across your infrastructure automatically.",backWhen:"Use when you want AI-automated evidence collection for compliance. Requires integration with your tech stack and evidence configuration. The AI agent approach reduces setup time compared to traditional integration-based tools."},
{name:"Scrut",url:"https://www.scrut.io/",category:"Compliance Automation",desc:"70+ integrations for continuous security compliance monitoring.",riskRaw:"Safe",audience:"Blue Team",tags:["Compliance","Integrations","Automation"],llm:[],stages:["govern","monitor"],agentic:false,complexityOverride:"Enterprise Only",backWhat:"A continuous security compliance monitoring platform with 70+ integrations. Provides real-time compliance status, automated evidence collection, and risk assessment across multiple frameworks.",backSecurity:"Continuous monitoring is essential for AI governance because AI systems change rapidly — model updates, new data sources, configuration changes. Scrut\'s real-time monitoring catches compliance drift as AI infrastructure evolves.",backWhen:"Deploy when you need continuous compliance monitoring with broad integration support. Enterprise platform requiring procurement, integration configuration, and organisation-wide adoption."}
];
const catMap={'AI Red Teaming':'cat-redteam','AI Governance & Standards':'cat-governance','AI Guardrails & Firewalls':'cat-guardrails','AI Development Tools':'cat-devtools','AI Code Assistants':'cat-codeassist','Foundation Models':'cat-models','Identity & AppSec':'cat-identity','Third-Party Risk':'cat-tprm','Compliance Automation':'cat-compliance'};
const initials=n=>{const w=n.replace(/[()]/g,'').split(/[\s-]+/);return w.length===1?w[0].substring(0,2).toUpperCase():(w[0][0]+w[1][0]).toUpperCase()};
function enrich(raw){return raw.map(r=>{const oi=parseTier(r.complexityOverride);let cx;if(oi!==null){cx={tier:TL[oi],tierKey:TK[oi],scores:{skill:0,deployment:0,governance:0,privacy:0},total:-1,overridden:true}}else{cx=assess(r)}return{...r,complexity:cx}})}
let RES=[],aCat='All',aAud='All',aTier='all',aRisk='All',aStage='All',sQ='';
function buildFilters(){
  const bar=document.getElementById('filterBar');bar.innerHTML='';
  const chevron='<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5"><path d="m6 9 6 6 6-6"/></svg>';
  function mkDrop(id,label,options,getter,setter){
    const wrap=document.createElement('div');wrap.className='filter-dropdown';wrap.id='dd-'+id;
    const cur=options.find(o=>o.key===getter())||options[0];
    const isAll=cur.key==='All'||cur.key==='all';
    wrap.innerHTML=`<div class="filter-trigger${isAll?'':' has-value'}" data-dd="${id}"><span class="filter-label">${label}</span><span class="filter-value">${isAll?'All':cur.label}</span>${chevron}</div><div class="filter-menu"></div>`;
    const menu=wrap.querySelector('.filter-menu');
    options.forEach(o=>{
      const opt=document.createElement('div');opt.className='filter-option'+(o.key===getter()?' active':'');
      opt.innerHTML=(o.code?`<span><span class="opt-code">${o.code}</span>${o.label}</span>`:`<span>${o.label}</span>`)+(o.count!=null?`<span class="opt-count">${o.count}</span>`:'');
      opt.onclick=(e)=>{e.stopPropagation();setter(o.key);closeAll();buildFilters();render();updateChips()};
      menu.appendChild(opt);
    });
    bar.appendChild(wrap);
  }
  const cats=[{key:'All',label:'All Categories',count:RES.length},...[...new Set(RES.map(r=>r.category))].sort().map(c=>({key:c,label:c,count:RES.filter(r=>r.category===c).length}))];
  mkDrop('cat','Category',cats,()=>aCat,v=>{aCat=v});
  mkDrop('role','Role',[{key:'All',label:'All Roles',count:RES.length},{key:'Blue Team',label:'Blue Team',count:RES.filter(r=>r.audience==='Blue Team').length},{key:'Red Team',label:'Red Team',count:RES.filter(r=>r.audience==='Red Team').length},{key:'Builder',label:'Builder',count:RES.filter(r=>r.audience==='Builder').length}],()=>aAud,v=>{aAud=v});
  mkDrop('tier','Expertise',[{key:'all',label:'All Levels',count:RES.length},{key:'plug-and-play',label:'Plug & Play',count:RES.filter(r=>r.complexity.tierKey==='plug-and-play').length},{key:'guided-setup',label:'Guided Setup',count:RES.filter(r=>r.complexity.tierKey==='guided-setup').length},{key:'expert-required',label:'Expert Required',count:RES.filter(r=>r.complexity.tierKey==='expert-required').length},{key:'enterprise-only',label:'Enterprise Only',count:RES.filter(r=>r.complexity.tierKey==='enterprise-only').length}],()=>aTier,v=>{aTier=v});
  mkDrop('risk','LLM Risk',[{key:'All',label:'All Risks',count:RES.length},...['LLM01','LLM02','LLM03','LLM04','LLM05','LLM06','LLM07','LLM08','LLM09','LLM10'].map(r=>{const n={'LLM01':'Prompt Injection','LLM02':'Insecure Output','LLM03':'Supply Chain','LLM04':'Data Poisoning','LLM05':'Improper Output','LLM06':'Info Disclosure','LLM07':'Insecure Plugin','LLM08':'Excessive Agency','LLM09':'Overreliance','LLM10':'Model Theft'};return{key:r,label:n[r],code:r,count:RES.filter(x=>(x.llm||[]).includes(r)).length}})],()=>aRisk,v=>{aRisk=v});
  const sl={scope:'Scope & Plan',augment:'Augment Data',develop:'Develop',test:'Test & Eval',release:'Release',deploy:'Deploy',operate:'Operate',monitor:'Monitor',govern:'Govern'};
  mkDrop('stage','Stage',[{key:'All',label:'All Stages',count:RES.length},...Object.entries(sl).map(([k,v])=>({key:k,label:v,count:RES.filter(x=>(x.stages||[]).includes(k)).length}))],()=>aStage,v=>{aStage=v});
  updateChips();
}
function closeAll(){document.querySelectorAll('.filter-dropdown.open').forEach(d=>d.classList.remove('open'))}
document.addEventListener('click',e=>{const trg=e.target.closest('.filter-trigger');if(trg){e.stopPropagation();const dd=trg.parentElement;const wasOpen=dd.classList.contains('open');closeAll();if(!wasOpen)dd.classList.add('open')}else{closeAll()}});
function updateChips(){
  const af=document.getElementById('activeFilters');af.innerHTML='';
  const x='<svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5"><path d="M18 6 6 18M6 6l12 12"/></svg>';
  if(aCat!=='All'){const c=document.createElement('span');c.className='active-chip';c.innerHTML=aCat+x;c.onclick=()=>{aCat='All';buildFilters();render()};af.appendChild(c)}
  if(aAud!=='All'){const c=document.createElement('span');c.className='active-chip';c.innerHTML=aAud+x;c.onclick=()=>{aAud='All';buildFilters();render()};af.appendChild(c)}
  if(aTier!=='all'){const lbl={'plug-and-play':'Plug & Play','guided-setup':'Guided Setup','expert-required':'Expert Required','enterprise-only':'Enterprise Only'};const c=document.createElement('span');c.className='active-chip';c.innerHTML=(lbl[aTier]||aTier)+x;c.onclick=()=>{aTier='all';buildFilters();render()};af.appendChild(c)}
  if(aRisk!=='All'){const c=document.createElement('span');c.className='active-chip';c.innerHTML=aRisk+x;c.onclick=()=>{aRisk='All';buildFilters();render()};af.appendChild(c)}
  if(aStage!=='All'){const sl={scope:'Scope & Plan',augment:'Augment Data',develop:'Develop',test:'Test & Eval',release:'Release',deploy:'Deploy',operate:'Operate',monitor:'Monitor',govern:'Govern'};const c=document.createElement('span');c.className='active-chip';c.innerHTML=(sl[aStage]||aStage)+x;c.onclick=()=>{aStage='All';buildFilters();render()};af.appendChild(c)}
}
function getF(){return RES.filter(r=>{if(aCat!=='All'&&r.category!==aCat)return false;if(aAud!=='All'&&r.audience!==aAud&&r.audience!=='All')return false;if(aTier!=='all'&&r.complexity.tierKey!==aTier)return false;if(aRisk!=='All'&&!(r.llm||[]).includes(aRisk))return false;if(aStage!=='All'&&!(r.stages||[]).includes(aStage))return false;if(sQ){const q=sQ.toLowerCase();return(r.name+' '+r.category+' '+r.desc+' '+r.tags.join(' ')+' '+r.audience+' '+r.complexity.tier+' '+(r.llm||[]).join(' ')).toLowerCase().includes(q)}return true})}
function mkCard(r){
  const cx=r.complexity,c=document.createElement('div');c.className=`card ${catMap[r.category]||''} tier-${cx.tierKey}`;c.style.cursor='pointer';
  c.addEventListener('click',function(e){e.preventDefault();showCardDetail(r)});
  const ag=r.agentic?'<span class="tag agentic">⚡ Agentic</span>':'';
  const rt=(r.llm||[]).length>0?r.llm.slice(0,3).map(l=>`<span class="tag llm-risk">${l}</span>`).join('')+(r.llm.length>3?`<span class="tag llm-risk">+${r.llm.length-3}</span>`:''):'';
  c.innerHTML=`<div class="card-top"><div class="card-identity"><div class="card-icon">${initials(r.name)}</div><div><div class="card-name">${r.name}</div><div class="card-category">${r.category}</div></div></div><span class="complexity-badge tier-${cx.tierKey}"><span class="badge-dot"></span>${cx.tier}</span></div><div class="card-desc">${r.desc}</div><div class="card-footer">${ag}${r.tags.slice(0,3).map(t=>`<span class="tag">${t}</span>`).join('')}${rt}</div>`;
  return c;
}
function render(){const g=document.getElementById('cardGrid'),f=getF();g.innerHTML='';f.forEach(r=>g.appendChild(mkCard(r)));document.getElementById('resultCount').textContent=`${f.length} of ${RES.length} resources`;document.getElementById('noResults').style.display=f.length?'none':'block'}
function stats(){document.getElementById('totalCount').textContent=RES.length;document.getElementById('catCount').textContent=new Set(RES.map(r=>r.category)).size;document.getElementById('tagCount').textContent=new Set(RES.flatMap(r=>r.tags)).size}
function banner(l,m){document.getElementById('sourceDot').className='dot '+(l?'live':'local');document.getElementById('sourceText').innerHTML=m;document.getElementById('sourceBanner').classList.add('active')}
function isQ(q){const l=q.toLowerCase().trim();return l.includes('?')||/^(what|which|how|why|where|who|can|do|does|is|are|show|find|list|recommend|suggest|help|tell|give|compare)/.test(l)}
function aiSearch(query){
  const ov=document.getElementById('aiOverlay'),body=document.getElementById('aiBody');ov.classList.add('active');
  body.innerHTML=`<div class="ai-loading"><div class="dots"><span></span><span></span><span></span></div>Searching ${RES.length} resources...</div>`;
  setTimeout(()=>{
    const q=query.toLowerCase(),kw=q.replace(/[?.,!]/g,'').split(/\s+/).filter(w=>w.length>2&&!['what','which','how','the','that','this','for','are','can','with','from','does','about','help','show','find','list','some','best','good','tools','tool','any','easy','simple'].includes(w));
    const scored=RES.map(r=>{let sc=0;const h=(r.name+' '+r.desc+' '+r.tags.join(' ')+' '+r.category+' '+r.audience+' '+r.complexity.tier+' '+(r.llm||[]).join(' ')).toLowerCase();kw.forEach(k=>{if(h.includes(k))sc+=h.split(k).length});r.tags.forEach(t=>{if(q.includes(t.toLowerCase()))sc+=3});
    if(q.includes('tprm')||q.includes('third-party')||q.includes('vendor risk'))if(r.category==='Third-Party Risk')sc+=5;
    if(q.includes('questionnaire')&&r.desc.toLowerCase().includes('questionnaire'))sc+=5;
    if(q.includes('compliance')&&(r.category==='Compliance Automation'||r.tags.some(t=>t.toLowerCase().includes('compliance'))))sc+=4;
    if(q.includes('red team')&&r.category==='AI Red Teaming')sc+=4;
    if((q.includes('guard')||q.includes('injection')||q.includes('firewall'))&&(r.category==='AI Guardrails & Firewalls'||r.tags.some(t=>t.toLowerCase().includes('guard')||t.toLowerCase().includes('firewall'))))sc+=5;
    if(q.includes('identity')&&r.category==='Identity & AppSec')sc+=4;
    if((q.includes('code')||q.includes('copilot'))&&r.category==='AI Code Assistants')sc+=4;
    if(q.includes('observ')||q.includes('trac')||q.includes('monitor'))if(r.tags.some(t=>t==='Observability'||t==='Tracing'))sc+=5;
    if((q.includes('beginner')||q.includes('easy'))&&r.complexity.tierKey==='plug-and-play')sc+=4;
    if(q.includes('governance')||q.includes('standard'))if(r.category==='AI Governance & Standards')sc+=4;
    if(q.includes('agent')||q.includes('agentic'))if(r.agentic)sc+=5;
    if(q.match(/llm0[1-9]|llm10/)){const m=q.match(/llm0[1-9]|llm10/g);m.forEach(lm=>{if((r.llm||[]).includes(lm.toUpperCase()))sc+=6})}
    return{...r,score:sc}}).filter(r=>r.score>0).sort((a,b)=>b.score-a.score).slice(0,6);
    if(!scored.length){body.innerHTML=`<div class="ai-response">No resources found matching "<strong>${query}</strong>". Try different keywords or use the LLM Top 10 filters.</div>`;return}
    const tc=[...new Set(scored.map(r=>r.category))].slice(0,2).join(' and ');
    body.innerHTML=`<div class="ai-response">Found <strong>${scored.length}</strong> resources for "<strong>${query}</strong>", primarily in ${tc}.</div><div class="ai-results">${scored.map(r=>`<a class="ai-result-card" href="${r.url}" target="_blank" rel="noopener"><div class="ai-result-icon ${catMap[r.category]||''}"><div class="card-icon" style="width:36px;height:36px;font-size:.7rem">${initials(r.name)}</div></div><div class="ai-result-info"><h4>${r.name}<span class="complexity-badge tier-${r.complexity.tierKey}" style="font-size:.55rem;padding:2px 6px;margin-left:4px"><span class="badge-dot"></span>${r.complexity.tier}</span>${r.agentic?'<span class="tag agentic" style="font-size:.55rem;padding:1px 6px">⚡</span>':''}</h4><p>${r.desc}</p></div></a>`).join('')}</div>`;
  },400);
}

const si=document.getElementById('searchInput'),ah=document.getElementById('aiHint');let db;
si.addEventListener('input',e=>{clearTimeout(db);const v=e.target.value.trim();ah.classList.toggle('visible',v.length>3&&isQ(v));db=setTimeout(()=>{sQ=v;if(!isQ(v))render()},200)});
si.addEventListener('keydown',e=>{if(e.key==='Enter'){const v=si.value.trim();if(v&&isQ(v))aiSearch(v);else{sQ=v;render()}}});
document.addEventListener('keydown',e=>{if((e.metaKey||e.ctrlKey)&&e.key==='k'){e.preventDefault();si.focus()}if(e.key==='Escape'){document.getElementById('aiOverlay').classList.remove('active');document.getElementById('configModal').classList.remove('active');si.blur()}});
document.getElementById('aiClose').onclick=()=>document.getElementById('aiOverlay').classList.remove('active');
document.getElementById('aiOverlay').onclick=e=>{if(e.target===e.currentTarget)e.currentTarget.classList.remove('active')};
document.querySelectorAll('.view-btn').forEach(b=>{b.onclick=()=>{document.querySelectorAll('.view-btn').forEach(x=>x.classList.remove('active'));b.classList.add('active');document.getElementById('cardGrid').classList.toggle('list-view',b.dataset.view==='list')}});
document.getElementById('configToggle').onclick=()=>document.getElementById('configModal').classList.add('active');
document.getElementById('cfgCancel').onclick=()=>document.getElementById('configModal').classList.remove('active');
document.getElementById('configModal').onclick=e=>{if(e.target===e.currentTarget)e.currentTarget.classList.remove('active')};
document.getElementById('cfgClear').onclick=()=>{clearAC();const s=document.getElementById('cfgStatus');s.className='config-status success';s.textContent='Cleared! Reloading...';setTimeout(()=>location.reload(),1000)};
document.getElementById('cfgSave').onclick=async()=>{const t=document.getElementById('cfgToken').value.trim(),b=document.getElementById('cfgBase').value.trim(),n=document.getElementById('cfgTable').value.trim()||'Resources',s=document.getElementById('cfgStatus');if(!t||!b){s.className='config-status error';s.textContent='Token and Base ID required.';return}s.className='config-status success';s.textContent='Testing...';try{const c={token:t,baseId:b,tableName:n},d=await fetchAT(c);saveAC(c);s.textContent=`Connected! ${d.length} records. Reloading...`;setTimeout(()=>location.reload(),1200)}catch(e){s.className='config-status error';s.textContent=`Failed: ${e.message}`}};
init();

function showCardDetail(tool){
  const overlay=document.getElementById('cardOverlay'),detail=document.getElementById('cardDetail');
  const catColors={'AI Red Teaming':'var(--red)','AI Governance & Standards':'var(--yellow)','AI Guardrails & Firewalls':'var(--green)','AI Development Tools':'var(--cyan)','AI Code Assistants':'var(--pink)','Foundation Models':'var(--purple)','Identity & AppSec':'#3b82f6','Third-Party Risk':'var(--orange)','Compliance Automation':'#14b8a6'};
  const color=catColors[tool.category]||'var(--accent)';
  const ini=initials(tool.name);
  let metaTags='';
  if(tool.complexity)metaTags+=`<span class="meta-tag">${tool.complexity.tier}</span>`;
  if(tool.audience&&tool.audience!=='All')metaTags+=`<span class="meta-tag">${tool.audience}</span>`;
  if(tool.agentic)metaTags+=`<span class="meta-tag">Agentic</span>`;
  (tool.llm||[]).forEach(r=>{metaTags+=`<span class="meta-tag">${r}</span>`});
  (tool.stages||[]).forEach(s=>{metaTags+=`<span class="meta-tag">${s}</span>`});
  detail.innerHTML=`<button class="back-close" onclick="closeCardDetail()" title="Close">&times;</button>
    <div class="back-header"><div class="back-avatar" style="background:${color}20;color:${color}">${ini}</div><div><div class="back-title">${tool.name}</div><div class="back-cat">${tool.category}</div></div></div>
    ${tool.backWhat?`<div class="back-section"><div class="back-section-title">What It Does</div><p>${tool.backWhat}</p></div>`:''}
    ${tool.backSecurity?`<div class="back-section"><div class="back-section-title">Security Relevance</div><p>${tool.backSecurity}</p></div>`:''}
    ${tool.backWhen?`<div class="back-section"><div class="back-section-title">When to Use It</div><p>${tool.backWhen}</p></div>`:''}
    <div class="back-meta">${metaTags}</div>
    <a href="${tool.url}" target="_blank" rel="noopener" class="back-link">Visit ${tool.name} <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2.5"><path d="M7 17L17 7M17 7H7M17 7V17"/></svg></a>`;
  overlay.classList.add('active');
  document.body.style.overflow='hidden';
}
function closeCardDetail(){
  document.getElementById('cardOverlay').classList.remove('active');
  document.body.style.overflow='';
}
document.getElementById('cardOverlay').addEventListener('click',function(e){if(e.target===this)closeCardDetail()});
document.addEventListener('keydown',function(e){if(e.key==='Escape')closeCardDetail()});