---
name: "Giskard"
url: "https://www.giskard.ai/"
category: "AI Red Teaming"
tier: "Guided Setup"
audience: "Builder"
risk: "Safe"
agentic: false
llm_risks: [LLM01, LLM02, LLM05]
stages: [test]
tags: 
  - Testing
  - Bias
  - Open Source
---

# Giskard

Open-source LLM testing for vulnerabilities, bias, and hallucination. Growing alternative to commercial red teaming.

## What It Does

An open-source testing framework for LLM vulnerabilities, bias, and hallucination. Provides automated test generation, vulnerability scanning, and continuous testing integration with a focus on both security and fairness.

## Security Relevance

Giskard uniquely combines security testing (prompt injection, data leakage) with fairness and bias evaluation. This dual focus is valuable because regulatory frameworks like the EU AI Act require both security and fairness assessments. One tool covering both reduces integration complexity.

## When to Use It

Use when you need to evaluate both security vulnerabilities and bias/fairness issues, particularly for EU AI Act compliance. Good for teams that want a growing open-source alternative to commercial red teaming platforms.
